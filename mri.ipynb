{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13218f9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 10:34:43.189212: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-04 10:34:43.308103: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/user2/anaconda3/envs/tf/lib/python3.10/site-packages/cv2/../../lib64:/usr/local/cuda-10.2/lib64\n",
      "2022-12-04 10:34:43.308123: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-12-04 10:34:44.240727: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/user2/anaconda3/envs/tf/lib/python3.10/site-packages/cv2/../../lib64:/usr/local/cuda-10.2/lib64\n",
      "2022-12-04 10:34:44.240860: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/user2/anaconda3/envs/tf/lib/python3.10/site-packages/cv2/../../lib64:/usr/local/cuda-10.2/lib64\n",
      "2022-12-04 10:34:44.240871: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import cv2\n",
    "import os\n",
    "from imutils import paths\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import cv2\n",
    "import sklearn\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = [16, 8]\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.layers import Conv2D,BatchNormalization,MaxPool2D,Concatenate,Add,Dropout,ReLU,Conv2DTranspose,UpSampling2D\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef26965",
   "metadata": {},
   "source": [
    "## Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7f6fbf94",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleMLP2:\n",
    "    @staticmethod\n",
    "    def build():\n",
    "        inputs = tf.keras.Input(shape=(image_size,image_size,1),name='input')\n",
    "        #inputs = keras.Input(shape=(128, 128, 1))\n",
    "        x = tf.keras.layers.Reshape((1, 448, 448, 1), input_shape=(None, 1, 448, 448 ,1))\n",
    "        #flat = keras.layers.Flatten()(x)\n",
    "\n",
    "        layer_0 = Conv2D(64,(7,7),strides=(2,2),padding='same',name='input_conv1',activation=None)(inputs)\n",
    "        layer = BatchNormalization(name='input_conv1bn')(layer_0)\n",
    "        layer = ReLU(name='input_conv1relu')(layer)\n",
    "\n",
    "        ## Encoder\n",
    "        layer = resblock(layer,'en_l1',64,keep_scale,l1l2,downsample=True,bn_act=True,first_layer=True)\n",
    "        layer = resblock(layer,'en_l2',64,keep_scale,l1l2,downsample=False,bn_act=True)\n",
    "        encoder_1 = resblock(layer,'en_l3',64,keep_scale,l1l2,downsample=False,bn_act=False)\n",
    "        layer = BatchNormalization(name='en_l3_finalbn')(encoder_1)\n",
    "        layer = ReLU(name='en_l3_finalrelu')(layer)\n",
    "\n",
    "        layer = resblock(layer,'en_l4',128,keep_scale,l1l2,downsample=True,bn_act=True)\n",
    "        layer = resblock(layer,'en_l5',128,keep_scale,l1l2,downsample=False,bn_act=True)\n",
    "        layer = resblock(layer,'en_l6',128,keep_scale,l1l2,downsample=False,bn_act=True)\n",
    "        encoder_2 = resblock(layer,'en_l7',128,keep_scale,l1l2,downsample=False,bn_act=False)\n",
    "        layer = BatchNormalization(name='en_l7_finalbn')(encoder_2)\n",
    "        layer = ReLU(name='en_l7_finalrelu')(layer)\n",
    "\n",
    "        layer = resblock(layer,'en_l8',256,keep_scale,l1l2,downsample=True,bn_act=True)\n",
    "        layer = resblock(layer,'en_l9',256,keep_scale,l1l2,downsample=False,bn_act=True)\n",
    "        layer = resblock(layer,'en_l10',256,keep_scale,l1l2,downsample=False,bn_act=True)\n",
    "        layer = resblock(layer,'en_l11',256,keep_scale,l1l2,downsample=False,bn_act=True)\n",
    "        layer = resblock(layer,'en_l12',256,keep_scale,l1l2,downsample=False,bn_act=True)\n",
    "        encoder_3 = resblock(layer,'en_l13',256,keep_scale,l1l2,downsample=False,bn_act=False)\n",
    "        layer = BatchNormalization(name='en_l13_finalbn')(encoder_3)\n",
    "        layer = ReLU(name='en_l13_finalrelu')(layer)\n",
    "\n",
    "        layer = resblock(layer,'en_l14',512,keep_scale,l1l2,downsample=True,bn_act=True)\n",
    "        layer = resblock(layer,'en_l15',512,keep_scale,l1l2,downsample=False,bn_act=True)\n",
    "        layer = resblock(layer,'en_l16',512,keep_scale,l1l2,downsample=False,bn_act=True)\n",
    "\n",
    "        ## DAC block\n",
    "        b1 = Conv2D(512,(3,3),padding='same',dilation_rate=1,name='dac_b1_conv1',activation=None)(layer)\n",
    "        # b1 = BatchNormalization()(b1)\n",
    "        # b1 = ReLU(name='dac_b1_relu')(b1)\n",
    "\n",
    "        b2 = Conv2D(512,(3,3),padding='same',dilation_rate=3,name='dac_b2_conv1',activation=None)(layer)\n",
    "        b2 = Conv2D(512,(1,1),padding='same',dilation_rate=1,name='dac_b2_conv2',activation=None)(b2)\n",
    "        # b2 = BatchNormalization()(b2)\n",
    "        # b2 = ReLU(name='dac_b2_relu')(b2)\n",
    "\n",
    "        b3 = Conv2D(512,(3,3),padding='same',dilation_rate=1,name='dac_b3_conv1',activation=None)(layer)\n",
    "        b3 = Conv2D(512,(3,3),padding='same',dilation_rate=3,name='dac_b3_conv2',activation=None)(b3)\n",
    "        b3 = Conv2D(512,(1,1),padding='same',dilation_rate=1,name='dac_b3_conv3',activation=None)(b3)\n",
    "        # b3 = BatchNormalization()(b3)\n",
    "        # b3 = ReLU(name='dac_b3_relu')(b3)\n",
    "\n",
    "        b4 = Conv2D(512,(3,3),padding='same',dilation_rate=1,name='dac_b4_conv1',activation=None)(layer)\n",
    "        b4 = Conv2D(512,(3,3),padding='same',dilation_rate=3,name='dac_b4_conv2',activation=None)(b4)\n",
    "        b4 = Conv2D(512,(3,3),padding='same',dilation_rate=5,name='dac_b4_conv3',activation=None)(b4)\n",
    "        b4 = Conv2D(512,(1,1),padding='same',dilation_rate=1,name='dac_b4_conv4',activation=None)(b4)\n",
    "        # b4 = BatchNormalization()(b4)\n",
    "        # b4 = ReLU(name='dac_b4_relu')(b4)\n",
    "\n",
    "        layer = Add(name='dac_add')([layer,b1,b2,b3,b4])\n",
    "        # layer = BatchNormalization(name='dac_bn')(layer)\n",
    "        layer = ReLU(name='dac_relu')(layer)\n",
    "\n",
    "        ## RMP block\n",
    "        b1 = MaxPool2D((2,2),strides=(2,2),padding='valid',name='rmp_b1_pool')(layer)\n",
    "        b1 = Conv2D(1,(1,1),padding='valid',name='rmb_b1_conv1',activation=None)(b1)\n",
    "        b1 = Conv2DTranspose(1,(1,1),(2,2),padding='valid',kernel_regularizer=l1l2,output_padding=0,activation=None)(b1)\n",
    "        b1 = tf.image.resize(b1, [14,14], method=tf.image.ResizeMethod.BILINEAR)\n",
    "\n",
    "        b2 = MaxPool2D((3,3),strides=(3,3),padding='valid',name='rmp_b2_pool')(layer)\n",
    "        b2 = Conv2D(1,(1,1),padding='valid',name='rmb_b2_conv1',activation=None)(b2)\n",
    "        b2 = Conv2DTranspose(1,(1,1),(3,3),padding='valid',kernel_regularizer=l1l2,output_padding=0,activation=None)(b2)\n",
    "        b2 = tf.image.resize(b2, [14,14], method=tf.image.ResizeMethod.BILINEAR)\n",
    "\n",
    "        b3 = MaxPool2D((5,5),strides=(5,5),padding='valid',name='rmp_b3_pool')(layer)\n",
    "        b3 = Conv2D(1,(1,1),padding='valid',name='rmb_b3_conv1',activation=None)(b3)\n",
    "        b3 = Conv2DTranspose(1,(1,1),(5,5),padding='valid',kernel_regularizer=l1l2,output_padding=0,activation=None)(b3)\n",
    "        b3 = tf.image.resize(b3, [14,14], method=tf.image.ResizeMethod.BILINEAR)\n",
    "\n",
    "        b4 = MaxPool2D((6,6),strides=(6,6),padding='valid',name='rmp_b4_pool')(layer)\n",
    "        b4 = Conv2D(1,(1,1),padding='valid',name='rmb_b4_conv1',activation=None)(b4)\n",
    "        b4 = Conv2DTranspose(1,(1,1),(6,6),padding='valid',kernel_regularizer=l1l2,output_padding=0,activation=None)(b4)\n",
    "        b4 = tf.image.resize(b4, [14,14], method=tf.image.ResizeMethod.BILINEAR)\n",
    "\n",
    "        layer = Concatenate(name='rmp_concat')([layer,b1,b2,b3,b4])\n",
    "        layer = ReLU(name='rmp_relu')(layer)\n",
    "\n",
    "        layer = Conv2D(256,(1,1),padding='same',kernel_regularizer=l1l2,name='de_l1_conv1',activation=None)(layer)\n",
    "        layer = BatchNormalization(name='de_l1_conv1bn')(layer)\n",
    "        layer = ReLU(name='de_l1_conv1relu')(layer)\n",
    "        layer = Conv2DTranspose(256,(3,3),(2,2),padding='same',kernel_regularizer=l1l2,output_padding=1,name='de_l1_deconv2',activation=None)(layer)\n",
    "        layer = BatchNormalization(name='de_l1_conv2bn')(layer)\n",
    "        layer = ReLU(name='de_l1_deconv2relu')(layer)\n",
    "        layer = Conv2D(256,(3,3),padding='same',kernel_regularizer=l1l2,name='de_l1_conv3',activation=None)(layer)\n",
    "        layer = Add(name='de_l1_add')([encoder_3*keep_scale,layer*(1-keep_scale)])\n",
    "        layer = BatchNormalization(name='de_l1_conv3bn')(layer)\n",
    "        layer = ReLU(name='de_l1_conv3relu')(layer)\n",
    "\n",
    "        layer = Conv2D(128,(1,1),padding='same',kernel_regularizer=l1l2,name='de_l2_conv1',activation=None)(layer)\n",
    "        layer = BatchNormalization(name='de_l2_conv1bn')(layer)\n",
    "        layer = ReLU(name='de_l2_conv1relu')(layer)\n",
    "        layer = Conv2DTranspose(128,(3,3),(2,2),padding='same',kernel_regularizer=l1l2,output_padding=1,name='de_l2_deconv2',activation=None)(layer)\n",
    "        layer = BatchNormalization(name='de_l2_conv2bn')(layer)\n",
    "        layer = ReLU(name='de_l2_deconv2relu')(layer)\n",
    "        layer = Conv2D(128,(1,1),padding='same',kernel_regularizer=l1l2,name='de_l2_conv3',activation=None)(layer)\n",
    "        layer = Add(name='de_l2_add')([encoder_2*keep_scale,layer*(1-keep_scale)])\n",
    "        layer = BatchNormalization(name='de_l2_conv3bn')(layer)\n",
    "        layer = ReLU(name='de_l2_conv3relu')(layer)\n",
    "\n",
    "        layer = Conv2D(64,(1,1),padding='same',kernel_regularizer=l1l2,name='de_l3_conv1',activation=None)(layer)\n",
    "        layer = BatchNormalization(name='de_l3_conv1bn')(layer)\n",
    "        layer = ReLU(name='de_l3_conv1relu')(layer)\n",
    "        layer = Conv2DTranspose(64,(3,3),(2,2),padding='same',kernel_regularizer=l1l2,output_padding=1,name='de_l3_deconv2',activation=None)(layer)\n",
    "        layer = BatchNormalization(name='de_l3_conv2bn')(layer)\n",
    "        layer = ReLU(name='de_l3_deconv2relu')(layer)\n",
    "        layer = Conv2D(64,(1,1),padding='same',kernel_regularizer=l1l2,name='de_l3_conv3',activation=None)(layer)\n",
    "        layer = Add(name='de_l3_add')([encoder_1*keep_scale,layer*(1-keep_scale)])\n",
    "        layer = BatchNormalization(name='de_l3_conv3bn')(layer)\n",
    "        layer = ReLU(name='de_l3_conv3relu')(layer)\n",
    "\n",
    "        layer = Conv2D(64,(1,1),padding='same',kernel_regularizer=l1l2,name='de_l4_conv1',activation=None)(layer)\n",
    "        layer = BatchNormalization(name='de_l4_conv1bn')(layer)\n",
    "        layer = ReLU(name='de_l4_conv1relu')(layer)\n",
    "        layer = Conv2DTranspose(64,(3,3),(2,2),padding='same',kernel_regularizer=l1l2,output_padding=1,name='de_l4_deconv2',activation=None)(layer)\n",
    "        layer = BatchNormalization(name='de_l4_conv2bn')(layer)\n",
    "        layer = ReLU(name='de_l4_deconv2relu')(layer)\n",
    "        layer = Conv2D(64,(1,1),padding='same',kernel_regularizer=l1l2,name='de_l4_conv3',activation=None)(layer)\n",
    "        layer = Add(name='de_l4_add')([layer_0*keep_scale,layer*(1-keep_scale)])\n",
    "        layer = BatchNormalization(name='de_l4_conv3bn')(layer)\n",
    "        layer = ReLU(name='de_l4_conv3relu')(layer)\n",
    "\n",
    "        layer = Conv2DTranspose(32,(3,3),(2,2),padding='same',kernel_regularizer=l1l2,name='final_deconv1',activation=None)(layer)\n",
    "        layer = BatchNormalization()(layer)\n",
    "        layer = ReLU()(layer)\n",
    "        layer = Conv2D(32,(3,3),padding='same',kernel_regularizer=l1l2,name='final_conv1',activation=None)(layer)\n",
    "        layer = BatchNormalization()(layer)\n",
    "        layer = ReLU()(layer)\n",
    "        outputs = Conv2D(1,(3,3),padding='same',name='output',activation=None)(layer)\n",
    "\n",
    "        model = tf.keras.Model(inputs,outputs,name='CE-Net')\n",
    "        #model.summary()\n",
    "        \n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5ea522eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_scale = 0.2\n",
    "l1l2 = tf.keras.regularizers.l1_l2(l1=0, l2=0.0005)\n",
    "\n",
    "def resblock(x,level='en_l1',filters=64,keep_scale=keep_scale,l1l2=l1l2,downsample=False,bn_act=True,first_layer=False):\n",
    "    if downsample:\n",
    "        if not first_layer:\n",
    "            x_H = Conv2D(filters,(3,3),(2,2),padding='same',kernel_regularizer=l1l2,name=level+'_Hconv')(x)\n",
    "            x = Conv2D(filters/2,(3,3),padding='same',kernel_regularizer=l1l2,name=level+'_conv1')(x)\n",
    "        else:\n",
    "            x_H = Conv2D(filters,(3,3),(2,2),padding='same',kernel_regularizer=l1l2,name=level+'_Hconv')(x)\n",
    "            x = Conv2D(filters,(3,3),padding='same',kernel_regularizer=l1l2,name=level+'_conv1')(x)\n",
    "    else:\n",
    "        x_H = x\n",
    "        x = Conv2D(filters,(3,3),padding='same',kernel_regularizer=l1l2,name=level+'_conv1')(x)\n",
    "    x = BatchNormalization(name=level+'_conv1bn')(x)\n",
    "    x = ReLU(name=level+'_conv1relu')(x)\n",
    "    if downsample:\n",
    "        x = Conv2D(filters,(3,3),(2,2),padding='same',kernel_regularizer=l1l2,name=level+'_conv2')(x)\n",
    "    else:\n",
    "        x = Conv2D(filters,(3,3),padding='same',kernel_regularizer=l1l2,name=level+'_conv2')(x)\n",
    "    x = BatchNormalization(name=level+'_conv2bn')(x)\n",
    "    x = ReLU(name=level+'_conv2relu')(x)\n",
    "    x = Add(name=level+'_add')([x_H*keep_scale,x*(1-keep_scale)])\n",
    "    if bn_act:\n",
    "        x = BatchNormalization(name=level+'_finalbn')(x)\n",
    "        x = ReLU(name=level+'_finalrelu')(x)       \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "2ed4add9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0001, 9.909954834128343e-05, 9.819818665965754e-05, 9.729590473501306e-05, 9.63926921258551e-05, 9.548853816214998e-05, 9.458343193786322e-05, 9.367736230317176e-05, 9.277031785633283e-05, 9.186228693518995e-05, 9.095325760829622e-05, 9.004321766563289e-05, 8.91321546089e-05, 8.822005564135439e-05, 8.73069076571686e-05, 8.639269723028191e-05, 8.547741060271343e-05, 8.45610336723042e-05, 8.36435519798534e-05, 8.272495069561094e-05, 8.180521460508584e-05, 8.088432809412662e-05, 7.996227513322693e-05, 7.903903926100555e-05, 7.811460356680608e-05, 7.718895067235705e-05, 7.626206271242883e-05, 7.533392131441787e-05, 7.440450757678327e-05, 7.347380204625457e-05]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1600x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lr_init = 0.0001\n",
    "\n",
    "def scheduler_1(epoch):\n",
    "    epoch += 1\n",
    "\n",
    "    if epoch <= 4:\n",
    "        return lr_init    \n",
    "\n",
    "    if epoch >= 5 and epoch <= 10:\n",
    "        return lr_init-lr_init*math.exp(0.25*(epoch-8))/40\n",
    "\n",
    "    elif epoch >= 11 and epoch <= 50:\n",
    "        return lr_init*math.exp(-0.05*(epoch-10))\n",
    "\n",
    "    else:\n",
    "        return scheduler_1(50-1)\n",
    "\n",
    "def scheduler_2(epoch):  \n",
    "    epoch += 1\n",
    "\n",
    "    if epoch == 1:\n",
    "        return lr_init\n",
    "\n",
    "    elif epoch >= 2 and epoch <= 35:\n",
    "        return (0.25*epoch**3)*math.exp(-0.25*epoch)*lr_init\n",
    "\n",
    "    else:\n",
    "        return scheduler_2(35-1)\n",
    "def scheduler_3(epoch):\n",
    "\n",
    "    if epoch == 0:\n",
    "        return lr_init\n",
    "\n",
    "    else:\n",
    "        return lr_init*((1-epoch/100)**0.9)\n",
    "\n",
    "stage = [i for i in range(0,30)]\n",
    "lr_plot = [scheduler_3(x) for x in stage]\n",
    "plt.plot(stage, lr_plot)\n",
    "print(lr_plot)\n",
    "\n",
    "smooth = K.epsilon()\n",
    "threshold = 0.8\n",
    "label_smoothing = 0.0\n",
    "bce_weight = 0.5\n",
    "\n",
    "def dice_coeff(y_true, y_pred):\n",
    "    numerator = tf.math.reduce_sum(y_true * y_pred) + smooth\n",
    "    denominator = tf.math.reduce_sum(y_true) + tf.math.reduce_sum(y_pred) + smooth\n",
    "    return tf.math.reduce_mean(2.* numerator / denominator) * 448 * 448\n",
    "\n",
    "def dice_loss(y_true, y_pred):\n",
    "    return - dice_coeff(y_true, y_pred)\n",
    "\n",
    "def bce_dice_loss(y_true, y_pred):\n",
    "    return bce_weight * tf.keras.losses.binary_crossentropy(y_true, y_pred, from_logits=False, label_smoothing=label_smoothing) - dice_coeff(y_true, y_pred) \n",
    "\n",
    "def bce_dice_loss_try(y_true, y_pred):\n",
    "    x = tf.keras.losses.binary_crossentropy(y_true, y_pred, from_logits=False, label_smoothing=label_smoothing)\n",
    "    y = -dice_coeff(y_true, y_pred)\n",
    "    return x + x/(x+y)*y\n",
    "\n",
    "def iou(y_true, y_pred):\n",
    "    overlap = tf.math.logical_and((y_true > threshold),(y_pred > threshold))\n",
    "    union = tf.math.logical_or((y_true > threshold),(y_pred > threshold))\n",
    "    iou = (tf.cast(tf.math.count_nonzero(overlap),tf.float32) + smooth) / (tf.cast(tf.math.count_nonzero(union),tf.float32) + smooth)\n",
    "    return iou\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee51972",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "63522b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = '/home/user2/Desktop/rumi/mri segmentation/2d_images'\n",
    "mask_path = '/home/user2/Desktop/rumi/mri segmentation/2d_masks'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c5d1c2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "autotune = tf.data.experimental.AUTOTUNE\n",
    "image_size = 448\n",
    "batch_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "23d350d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    }
   ],
   "source": [
    "exclude = [image_path+'/ID_0254_Z_0075.tif',mask_path+'/ID_0254_Z_0075.tif',\n",
    "           image_path+'/ID_0052_Z_0108.tif',mask_path+'/ID_0052_Z_0108.tif',\n",
    "           image_path+'/ID_0079_Z_0072.tif',mask_path+'/ID_0079_Z_0072.tif',\n",
    "           image_path+'/ID_0134_Z_0137.tif',mask_path+'/ID_0134_Z_0137.tif']\n",
    "num_files = 50\n",
    "\n",
    "def read_image(image):\n",
    "    path = image.numpy().decode('utf-8')\n",
    "    if path not in exclude:\n",
    "        image = cv2.imread(path,0)\n",
    "        image = np.expand_dims(image,2)\n",
    "        image = tf.image.resize(image, [image_size,image_size])\n",
    "    else:\n",
    "        image = np.zeros((image_size,image_size,1),dtype=np.float32)\n",
    "    return image\n",
    "\n",
    "def read_mask(image):\n",
    "    path = image.numpy().decode('utf-8')\n",
    "    if path not in exclude:\n",
    "        image = cv2.imread(path,0)\n",
    "        image = image*257.\n",
    "        image = np.expand_dims(image,2)\n",
    "        image = tf.image.resize(image, [image_size,image_size])\n",
    "    else:\n",
    "        image = np.zeros((image_size,image_size,1),dtype=np.float32)\n",
    "    return image\n",
    "\n",
    "def aug(image,label):\n",
    "    seed = np.random.randint(0,64)\n",
    "    image = tf.image.random_flip_left_right(image, seed=seed)\n",
    "    image = tf.image.random_flip_up_down(image, seed=seed)\n",
    "    image = tf.image.random_contrast(image, 0.3, 0.7, seed=seed)\n",
    "    label = tf.image.random_flip_left_right(label, seed=seed)\n",
    "    label = tf.image.random_flip_up_down(label, seed=seed)\n",
    "    label = tf.image.random_contrast(label, 0.3, 0.7, seed=seed)\n",
    "    return image,label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed29758b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c649489c",
   "metadata": {},
   "source": [
    "#  Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a4e2d0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_files=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "b64aa796",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset_image1 = tf.data.Dataset.list_files('/home/user2/Desktop/rumi/mri segmentation2/2d_images/client1/*.tif',shuffle=False,seed=None).map(lambda x: tf.py_function(read_image,[x],[tf.float32]),num_parallel_calls=autotune,deterministic=False)\n",
    "dataset_mask1 = tf.data.Dataset.list_files('/home/user2/Desktop/rumi/mri segmentation2/2d_masks/client1/*.tif',shuffle=False,seed=None).map(lambda x: tf.py_function(read_mask,[x],[tf.float32]),num_parallel_calls=autotune,deterministic=False)\n",
    "\n",
    "dataset_image2 = tf.data.Dataset.list_files('/home/user2/Desktop/rumi/mri segmentation2/2d_images/client2/*.tif',shuffle=False,seed=None).map(lambda x: tf.py_function(read_image,[x],[tf.float32]),num_parallel_calls=autotune,deterministic=False)\n",
    "dataset_mask2 = tf.data.Dataset.list_files('/home/user2/Desktop/rumi/mri segmentation2/2d_masks/client2/*.tif',shuffle=False,seed=None).map(lambda x: tf.py_function(read_mask,[x],[tf.float32]),num_parallel_calls=autotune,deterministic=False)\n",
    "\n",
    "dataset_image3 = tf.data.Dataset.list_files('/home/user2/Desktop/rumi/mri segmentation2/2d_images/client3/*.tif',shuffle=False,seed=None).map(lambda x: tf.py_function(read_image,[x],[tf.float32]),num_parallel_calls=autotune,deterministic=False)\n",
    "dataset_mask3 = tf.data.Dataset.list_files('/home/user2/Desktop/rumi/mri segmentation2/2d_masks/client3/*.tif',shuffle=False,seed=None).map(lambda x: tf.py_function(read_mask,[x],[tf.float32]),num_parallel_calls=autotune,deterministic=False)\n",
    "\n",
    "dataset_image4 = tf.data.Dataset.list_files('/home/user2/Desktop/rumi/mri segmentation2/2d_images/client4/*.tif',shuffle=False,seed=None).map(lambda x: tf.py_function(read_image,[x],[tf.float32]),num_parallel_calls=autotune,deterministic=False)\n",
    "dataset_mask4 = tf.data.Dataset.list_files('/home/user2/Desktop/rumi/mri segmentation2/2d_masks/client4/*.tif',shuffle=False,seed=None).map(lambda x: tf.py_function(read_mask,[x],[tf.float32]),num_parallel_calls=autotune,deterministic=False)\n",
    "\n",
    "dataset_image5 = tf.data.Dataset.list_files('/home/user2/Desktop/rumi/mri segmentation2/2d_images/client5/*.tif',shuffle=False,seed=None).map(lambda x: tf.py_function(read_image,[x],[tf.float32]),num_parallel_calls=autotune,deterministic=False)\n",
    "dataset_mask5 = tf.data.Dataset.list_files('/home/user2/Desktop/rumi/mri segmentation2/2d_masks/client5/*.tif',shuffle=False,seed=None).map(lambda x: tf.py_function(read_mask,[x],[tf.float32]),num_parallel_calls=autotune,deterministic=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "037b8c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset1 = tf.data.Dataset.zip((dataset_image1, dataset_mask1))\n",
    "dataset1 = dataset1.repeat().shuffle(21).batch(batch_size).cache().prefetch(autotune)\n",
    "train1 = dataset1.take(int(0.8*num_files//batch_size))\n",
    "test1 = dataset1.skip(int(0.8*num_files//batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "b4a868c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset2 = tf.data.Dataset.zip((dataset_image2, dataset_mask2))\n",
    "dataset2 = dataset2.repeat().shuffle(21).batch(batch_size).cache().prefetch(autotune)\n",
    "train2 = dataset2.take(int(0.8*num_files//batch_size))\n",
    "test2 = dataset2.skip(int(0.8*num_files//batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "89abec35",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset3 = tf.data.Dataset.zip((dataset_image3, dataset_mask3))\n",
    "dataset3 = dataset3.repeat().shuffle(21).batch(batch_size).cache().prefetch(autotune)\n",
    "train3 = dataset3.take(int(0.8*num_files//batch_size))\n",
    "test3 = dataset3.skip(int(0.8*num_files//batch_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "c291a3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset4 = tf.data.Dataset.zip((dataset_image4, dataset_mask4))\n",
    "dataset4 = dataset4.repeat().shuffle(21).batch(batch_size).cache().prefetch(autotune)\n",
    "train4 = dataset4.take(int(0.8*num_files//batch_size))\n",
    "test4 = dataset4.skip(int(0.8*num_files//batch_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "2837b888",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset5 = tf.data.Dataset.zip((dataset_image5, dataset_mask5))\n",
    "dataset5 = dataset5.repeat().shuffle(21).batch(batch_size).cache().prefetch(autotune)\n",
    "train5 = dataset5.take(int(0.8*num_files//batch_size))\n",
    "test5 = dataset5.skip(int(0.8*num_files//batch_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "0fd383f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "trains=[train1, train2, train3, train4, train5]\n",
    "tests=[test1, test2, test3, test4, test5]\n",
    "\n",
    "#trains=[train1]\n",
    "#tests=[test1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae80ab6",
   "metadata": {},
   "source": [
    "# Model initialization and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "9093310d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_scalling_factor(clients_trn_data, client_name):\n",
    "    client_names = list(clients_trn_data.keys())\n",
    "    #get the bs\n",
    "    bs = list(clients_trn_data[client_name])[0][0].shape[0]\n",
    "    #first calculate the total training data points across clinets\n",
    "    global_count = sum([tf.data.experimental.cardinality(clients_trn_data[client_name]).numpy() for client_name in client_names])*bs\n",
    "    # get the total number of data points held by a client\n",
    "    local_count = tf.data.experimental.cardinality(clients_trn_data[client_name]).numpy()*bs\n",
    "    return local_count/global_count\n",
    "\n",
    "\n",
    "def scale_model_weights(weight, scalar):\n",
    "    '''function for scaling a models weights'''\n",
    "    weight_final = []\n",
    "    steps = len(weight)\n",
    "    for i in range(steps):\n",
    "        weight_final.append(scalar * weight[i])\n",
    "    return weight_final\n",
    "\n",
    "\n",
    "\n",
    "def sum_scaled_weights(scaled_weight_list):\n",
    "    '''Return the sum of the listed scaled weights. The is equivalent to scaled avg of the weights'''\n",
    "    avg_grad = list()\n",
    "    #get the average grad accross all client gradients\n",
    "    for grad_list_tuple in zip(*scaled_weight_list):\n",
    "        layer_mean = tf.math.reduce_sum(grad_list_tuple, axis=0)\n",
    "        avg_grad.append(layer_mean)\n",
    "        \n",
    "    return avg_grad\n",
    "\n",
    "\n",
    "def test_model(X_test, Y_test,  model, comm_round):\n",
    "    cce = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "    #logits = model.predict(X_test, batch_size=100)\n",
    "    logits = model.predict(X_test)\n",
    "    loss = cce(Y_test, logits)\n",
    "    acc = accuracy_score(tf.argmax(logits, axis=1), tf.argmax(Y_test, axis=1))\n",
    "    print('comm_round: {} | global_acc: {:.3%} | global_loss: {}'.format(comm_round, acc, loss))\n",
    "    return acc, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "75a10503",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = bce_dice_loss\n",
    "optimizer = tf.keras.optimizers.Adam(lr_init)\n",
    "metrics = [tf.keras.metrics.BinaryCrossentropy(from_logits=False,label_smoothing=label_smoothing,dtype=tf.float32,name='bce'), dice_coeff, iou]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "0ac392d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "SCALINGFACTOR=0.2\n",
    "EPOCH=5\n",
    "smlp_SGD2 = SimpleMLP2()\n",
    "SGD_model2 = smlp_SGD2.build() \n",
    "SGD_model2.compile(loss=loss, \n",
    "              optimizer=optimizer, \n",
    "              metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "246eb9f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for client  0\n",
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 0.0001.\n",
      "Epoch 1/5\n",
      "3/4 [=====================>........] - ETA: 15s - loss: 221915.9375 - bce: 157458.8125 - dice_coeff: -143181.7656 - iou: 0.0271"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 18:46:16.402070: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 88s 17s/step - loss: 208039.9531 - bce: 148589.1406 - dice_coeff: -133740.5938 - iou: 0.0263 - val_loss: 136436.4844 - val_bce: 87663.8906 - val_dice_coeff: -92599.8594 - val_iou: 0.1386 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 2: LearningRateScheduler setting learning rate to 9.909954834128343e-05.\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 18:46:36.953350: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/4 [=====================>........] - ETA: 14s - loss: 160510.4688 - bce: 127091.4375 - dice_coeff: -96960.1797 - iou: 0.0295 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 18:47:22.244819: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 64s 16s/step - loss: 157193.5781 - bce: 129124.8672 - dice_coeff: -92626.5312 - iou: 0.0301 - val_loss: 64135.8359 - val_bce: 86830.4297 - val_dice_coeff: -20716.0293 - val_iou: 0.1466 - lr: 9.9100e-05\n",
      "\n",
      "Epoch 3: LearningRateScheduler setting learning rate to 9.819818665965754e-05.\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 18:47:40.865596: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/4 [=====================>........] - ETA: 14s - loss: 92282.2578 - bce: 112834.8438 - dice_coeff: -35860.2305 - iou: 0.0267 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 18:48:25.966703: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 63s 16s/step - loss: 105552.8359 - bce: 112222.7031 - dice_coeff: -49436.8555 - iou: 0.0317 - val_loss: 30281.5371 - val_bce: 60861.0938 - val_dice_coeff: 153.6343 - val_iou: 0.1412 - lr: 9.8198e-05\n",
      "\n",
      "Epoch 4: LearningRateScheduler setting learning rate to 9.729590473501306e-05.\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 18:48:44.430808: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/4 [=====================>........] - ETA: 14s - loss: 58884.3008 - bce: 92516.3125 - dice_coeff: -12621.4990 - iou: 0.0683"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 18:49:29.487671: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 64s 16s/step - loss: 63299.8047 - bce: 93792.0000 - dice_coeff: -16399.1680 - iou: 0.0727 - val_loss: 11484.9092 - val_bce: 62925.6328 - val_dice_coeff: 19982.5332 - val_iou: 0.1305 - lr: 9.7296e-05\n",
      "\n",
      "Epoch 5: LearningRateScheduler setting learning rate to 9.63926921258551e-05.\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 18:49:47.997781: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/4 [=====================>........] - ETA: 14s - loss: 18557.5879 - bce: 80957.9297 - dice_coeff: 21926.0176 - iou: 0.1175"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 18:50:32.903414: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 64s 16s/step - loss: 2730.2695 - bce: 73229.4375 - dice_coeff: 33889.1016 - iou: 0.1235 - val_loss: -28833.9883 - val_bce: 56008.7695 - val_dice_coeff: 56842.9961 - val_iou: 0.1395 - lr: 9.6393e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 18:50:51.814279: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for client  1\n",
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 0.0001.\n",
      "Epoch 1/5\n",
      "3/4 [=====================>........] - ETA: 15s - loss: -37784.9531 - bce: 51609.9766 - dice_coeff: 63594.5781 - iou: 0.1490"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 18:51:44.754358: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 71s 17s/step - loss: -35498.2734 - bce: 49708.0664 - dice_coeff: 60356.9453 - iou: 0.1537 - val_loss: -102392.0000 - val_bce: 27826.8770 - val_dice_coeff: 116310.0547 - val_iou: 0.1468 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 2: LearningRateScheduler setting learning rate to 9.909954834128343e-05.\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 18:52:04.312605: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/4 [=====================>........] - ETA: 14s - loss: -67569.6641 - bce: 33180.1055 - dice_coeff: 84164.3906 - iou: 0.1948"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 18:52:49.789636: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 64s 16s/step - loss: -71520.0859 - bce: 28391.7773 - dice_coeff: 85720.6406 - iou: 0.1954 - val_loss: -129991.7656 - val_bce: 30221.9492 - val_dice_coeff: 145107.2656 - val_iou: 0.1400 - lr: 9.9100e-05\n",
      "\n",
      "Epoch 3: LearningRateScheduler setting learning rate to 9.819818665965754e-05.\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 18:53:08.328849: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/4 [=====================>........] - ETA: 16s - loss: -129867.2500 - bce: -15691.7295 - dice_coeff: 122025.9609 - iou: 0.1950"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 18:53:56.022967: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 70s 18s/step - loss: -147549.2031 - bce: -20709.7910 - dice_coeff: 137198.9688 - iou: 0.2002 - val_loss: -145892.5469 - val_bce: 39675.1680 - val_dice_coeff: 165734.7500 - val_iou: 0.1390 - lr: 9.8198e-05\n",
      "\n",
      "Epoch 4: LearningRateScheduler setting learning rate to 9.729590473501306e-05.\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 18:54:18.352369: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/4 [=====================>........] - ETA: 15s - loss: -197153.7969 - bce: -40079.1992 - dice_coeff: 177118.9375 - iou: 0.2160"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 18:55:05.792316: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 67s 17s/step - loss: -201323.7344 - bce: -40943.2422 - dice_coeff: 180856.7812 - iou: 0.2208 - val_loss: -179760.5312 - val_bce: 41326.0820 - val_dice_coeff: 200428.2500 - val_iou: 0.1441 - lr: 9.7296e-05\n",
      "\n",
      "Epoch 5: LearningRateScheduler setting learning rate to 9.63926921258551e-05.\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 18:55:25.372011: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/4 [=====================>........] - ETA: 15s - loss: -204779.5000 - bce: -41146.8633 - dice_coeff: 184210.7500 - iou: 0.2351"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 18:56:13.425490: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "4/4 [==============================] - ETA: 0s - loss: -218341.8281 - bce: -44560.1055 - dice_coeff: 196066.4688 - iou: 0.2354 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 18:56:33.155550: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 69s 18s/step - loss: -218341.8281 - bce: -44560.1055 - dice_coeff: 196066.4688 - iou: 0.2354 - val_loss: -226178.9688 - val_bce: 32025.5566 - val_dice_coeff: 242196.5938 - val_iou: 0.1433 - lr: 9.6393e-05\n",
      "for client  2\n",
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 0.0001.\n",
      "Epoch 1/5\n",
      "3/4 [=====================>........] - ETA: 16s - loss: -234323.1719 - bce: -55480.0820 - dice_coeff: 206587.6406 - iou: 0.2278"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 18:57:31.696134: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 75s 18s/step - loss: -247300.3281 - bce: -66196.9141 - dice_coeff: 214206.3125 - iou: 0.2456 - val_loss: -282811.0938 - val_bce: 18759.8770 - val_dice_coeff: 292195.3125 - val_iou: 0.2040 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 2: LearningRateScheduler setting learning rate to 9.909954834128343e-05.\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 18:57:51.539378: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/4 [=====================>........] - ETA: 15s - loss: -261220.1719 - bce: -77195.0391 - dice_coeff: 222627.1719 - iou: 0.2479"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 18:58:38.279525: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 66s 17s/step - loss: -273417.3125 - bce: -79660.7031 - dice_coeff: 233591.4688 - iou: 0.2448 - val_loss: -338215.2812 - val_bce: -40017.5352 - val_dice_coeff: 318211.2500 - val_iou: 0.1752 - lr: 9.9100e-05\n",
      "\n",
      "Epoch 3: LearningRateScheduler setting learning rate to 9.819818665965754e-05.\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 18:58:57.570883: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/4 [=====================>........] - ETA: 15s - loss: -345272.0938 - bce: -109489.9609 - dice_coeff: 290531.3438 - iou: 0.2534"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 18:59:43.857727: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 65s 17s/step - loss: -380184.0000 - bce: -124727.7891 - dice_coeff: 317824.3438 - iou: 0.2683 - val_loss: -351159.7500 - val_bce: -43031.8789 - val_dice_coeff: 329648.7812 - val_iou: 0.2061 - lr: 9.8198e-05\n",
      "\n",
      "Epoch 4: LearningRateScheduler setting learning rate to 9.729590473501306e-05.\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 19:00:02.822385: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/4 [=====================>........] - ETA: 15s - loss: -396657.4375 - bce: -130768.6328 - dice_coeff: 331277.9062 - iou: 0.2672"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 19:00:49.579271: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 66s 17s/step - loss: -399949.0625 - bce: -128397.7500 - dice_coeff: 335755.0312 - iou: 0.2606 - val_loss: -427057.6875 - val_bce: -56813.7383 - val_dice_coeff: 398654.8750 - val_iou: 0.1995 - lr: 9.7296e-05\n",
      "\n",
      "Epoch 5: LearningRateScheduler setting learning rate to 9.63926921258551e-05.\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 19:01:08.536312: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/4 [=====================>........] - ETA: 15s - loss: -436387.1250 - bce: -137039.1250 - dice_coeff: 367872.6250 - iou: 0.2731"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 19:01:54.894820: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 65s 17s/step - loss: -448275.4062 - bce: -141354.6875 - dice_coeff: 377603.1562 - iou: 0.2741 - val_loss: -573258.3125 - val_bce: -104699.3984 - val_dice_coeff: 520913.6875 - val_iou: 0.2880 - lr: 9.6393e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 19:02:13.914126: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for client  3\n",
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 0.0001.\n",
      "Epoch 1/5\n",
      "3/4 [=====================>........] - ETA: 15s - loss: -441970.5938 - bce: -117699.5234 - dice_coeff: 383125.8438 - iou: 0.2297"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 19:03:09.861782: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 75s 17s/step - loss: -412155.9375 - bce: -111296.0781 - dice_coeff: 356512.7812 - iou: 0.2224 - val_loss: -573438.6250 - val_bce: -122500.0625 - val_dice_coeff: 512193.8438 - val_iou: 0.2068 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 2: LearningRateScheduler setting learning rate to 9.909954834128343e-05.\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 19:03:30.728876: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/4 [=====================>........] - ETA: 15s - loss: -427713.7188 - bce: -120406.5078 - dice_coeff: 367515.2188 - iou: 0.2407"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 19:04:17.754098: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 66s 17s/step - loss: -434826.3438 - bce: -117074.4688 - dice_coeff: 376293.6250 - iou: 0.2297 - val_loss: -579598.8125 - val_bce: -121801.3047 - val_dice_coeff: 518702.1250 - val_iou: 0.2099 - lr: 9.9100e-05\n",
      "\n",
      "Epoch 3: LearningRateScheduler setting learning rate to 9.819818665965754e-05.\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 19:04:37.062847: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/4 [=====================>........] - ETA: 16s - loss: -460964.0625 - bce: -119656.7109 - dice_coeff: 401140.1250 - iou: 0.2319"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 19:05:25.056742: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 68s 17s/step - loss: -447229.1562 - bce: -116469.8125 - dice_coeff: 388998.5312 - iou: 0.2287 - val_loss: -647519.0625 - val_bce: -148905.5938 - val_dice_coeff: 573070.6250 - val_iou: 0.2380 - lr: 9.8198e-05\n",
      "\n",
      "Epoch 4: LearningRateScheduler setting learning rate to 9.729590473501306e-05.\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 19:05:44.697270: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/4 [=====================>........] - ETA: 15s - loss: -555377.8750 - bce: -138791.9062 - dice_coeff: 485986.3750 - iou: 0.2457"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 19:06:31.978251: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 67s 17s/step - loss: -556381.6875 - bce: -136244.7500 - dice_coeff: 488263.9375 - iou: 0.2415 - val_loss: -596781.6875 - val_bce: -110876.0156 - val_dice_coeff: 541347.6250 - val_iou: 0.1863 - lr: 9.7296e-05\n",
      "\n",
      "Epoch 5: LearningRateScheduler setting learning rate to 9.63926921258551e-05.\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 19:06:51.360776: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/4 [=====================>........] - ETA: 15s - loss: -483100.1875 - bce: -114841.3281 - dice_coeff: 425683.7188 - iou: 0.2226"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 19:07:38.974191: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 67s 17s/step - loss: -523287.6875 - bce: -121958.1250 - dice_coeff: 462313.1250 - iou: 0.2238 - val_loss: -729775.3125 - val_bce: -142618.6250 - val_dice_coeff: 658469.6875 - val_iou: 0.2154 - lr: 9.6393e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 19:07:58.335576: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for client  4\n",
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 0.0001.\n",
      "Epoch 1/5\n",
      "3/4 [=====================>........] - ETA: 15s - loss: -616008.6875 - bce: -150927.4375 - dice_coeff: 540549.5000 - iou: 0.2712"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 19:08:53.438832: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 73s 17s/step - loss: -603100.8750 - bce: -143376.6875 - dice_coeff: 531417.0625 - iou: 0.2584 - val_loss: -831944.0000 - val_bce: -197780.8594 - val_dice_coeff: 733057.5625 - val_iou: 0.2816 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 2: LearningRateScheduler setting learning rate to 9.909954834128343e-05.\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 19:09:13.373961: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/4 [=====================>........] - ETA: 15s - loss: -651111.8750 - bce: -145699.4219 - dice_coeff: 578267.0625 - iou: 0.2569"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 19:10:00.154779: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 66s 16s/step - loss: -643290.9375 - bce: -137466.7031 - dice_coeff: 574562.6250 - iou: 0.2422 - val_loss: -789287.6250 - val_bce: -138066.8281 - val_dice_coeff: 720259.3750 - val_iou: 0.2194 - lr: 9.9100e-05\n",
      "\n",
      "Epoch 3: LearningRateScheduler setting learning rate to 9.819818665965754e-05.\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 19:10:19.167541: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/4 [=====================>........] - ETA: 15s - loss: -801248.0000 - bce: -168982.9062 - dice_coeff: 716761.3125 - iou: 0.2746"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 19:11:05.630572: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 66s 17s/step - loss: -779352.5625 - bce: -164012.6562 - dice_coeff: 697350.9375 - iou: 0.2736 - val_loss: -863096.1250 - val_bce: -175372.5000 - val_dice_coeff: 775413.6875 - val_iou: 0.2538 - lr: 9.8198e-05\n",
      "\n",
      "Epoch 4: LearningRateScheduler setting learning rate to 9.729590473501306e-05.\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 19:11:24.783135: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/4 [=====================>........] - ETA: 15s - loss: -778006.5625 - bce: -158487.9688 - dice_coeff: 698767.1875 - iou: 0.2679"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 19:12:11.103671: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 65s 17s/step - loss: -780529.1250 - bce: -151817.8906 - dice_coeff: 704624.6250 - iou: 0.2566 - val_loss: -902529.3750 - val_bce: -188433.3438 - val_dice_coeff: 808318.7500 - val_iou: 0.2753 - lr: 9.7296e-05\n",
      "\n",
      "Epoch 5: LearningRateScheduler setting learning rate to 9.63926921258551e-05.\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 19:12:30.200809: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/4 [=====================>........] - ETA: 15s - loss: -859307.3750 - bce: -154553.2500 - dice_coeff: 782034.7500 - iou: 0.2560"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 19:13:16.795612: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 66s 17s/step - loss: -851054.0000 - bce: -153081.4062 - dice_coeff: 774517.5000 - iou: 0.2547 - val_loss: -994940.1250 - val_bce: -195848.2188 - val_dice_coeff: 897021.5625 - val_iou: 0.2758 - lr: 9.6393e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 19:13:36.293813: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for client  0\n",
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 0.0001.\n",
      "Epoch 1/5\n",
      "3/4 [=====================>........] - ETA: 15s - loss: -973541.9375 - bce: -177623.2969 - dice_coeff: 884735.3125 - iou: 0.2841"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 19:14:47.420798: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 73s 17s/step - loss: -994282.1875 - bce: -178112.9844 - dice_coeff: 905230.7500 - iou: 0.2842 - val_loss: -1009908.1250 - val_bce: -206787.6250 - val_dice_coeff: 906518.7500 - val_iou: 0.2770 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 2: LearningRateScheduler setting learning rate to 9.909954834128343e-05.\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 19:15:07.535532: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/4 [=====================>........] - ETA: 15s - loss: -1015131.7500 - bce: -164516.8281 - dice_coeff: 932878.1875 - iou: 0.2744"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 19:15:54.811935: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 67s 17s/step - loss: -989473.8125 - bce: -160256.9219 - dice_coeff: 909350.4375 - iou: 0.2701 - val_loss: -944756.8750 - val_bce: -177336.3281 - val_dice_coeff: 856095.5625 - val_iou: 0.2505 - lr: 9.9100e-05\n",
      "\n",
      "Epoch 3: LearningRateScheduler setting learning rate to 9.819818665965754e-05.\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 19:16:14.755713: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/4 [=====================>........] - ETA: 15s - loss: -1064423.1250 - bce: -167107.7656 - dice_coeff: 980873.8125 - iou: 0.2843"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 19:17:01.270500: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 66s 17s/step - loss: -1091407.6250 - bce: -165418.7969 - dice_coeff: 1008702.6875 - iou: 0.2884 - val_loss: -1029449.8750 - val_bce: -172932.9375 - val_dice_coeff: 942989.3750 - val_iou: 0.2294 - lr: 9.8198e-05\n",
      "\n",
      "Epoch 4: LearningRateScheduler setting learning rate to 9.729590473501306e-05.\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 19:17:20.369656: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/4 [=====================>........] - ETA: 15s - loss: -1190730.3750 - bce: -175496.6562 - dice_coeff: 1102986.6250 - iou: 0.3452"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 19:18:06.439068: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 65s 17s/step - loss: -1200309.3750 - bce: -177698.6875 - dice_coeff: 1111464.7500 - iou: 0.3540 - val_loss: -1098621.2500 - val_bce: -165086.4375 - val_dice_coeff: 1016080.5625 - val_iou: 0.2142 - lr: 9.7296e-05\n",
      "\n",
      "Epoch 5: LearningRateScheduler setting learning rate to 9.63926921258551e-05.\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 19:18:25.694136: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/4 [=====================>........] - ETA: 15s - loss: -1206001.3750 - bce: -167281.5938 - dice_coeff: 1122364.0000 - iou: 0.3641"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 19:19:12.344029: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 66s 17s/step - loss: -1226562.3750 - bce: -163385.2031 - dice_coeff: 1144873.2500 - iou: 0.3712 - val_loss: -1107690.5000 - val_bce: -197107.0156 - val_dice_coeff: 1009140.3750 - val_iou: 0.2504 - lr: 9.6393e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 19:19:31.459529: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for client  1\n",
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 0.0001.\n",
      "Epoch 1/5\n",
      "3/4 [=====================>........] - ETA: 15s - loss: -1318737.3750 - bce: -175890.6094 - dice_coeff: 1230795.8750 - iou: 0.4560"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 19:20:25.867212: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 73s 17s/step - loss: -1319550.7500 - bce: -174590.3750 - dice_coeff: 1232260.0000 - iou: 0.4533 - val_loss: -1057565.6250 - val_bce: -212211.3594 - val_dice_coeff: 951465.2500 - val_iou: 0.2698 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 2: LearningRateScheduler setting learning rate to 9.909954834128343e-05.\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 19:20:45.890266: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/4 [=====================>........] - ETA: 15s - loss: -1432370.0000 - bce: -173171.7031 - dice_coeff: 1345788.5000 - iou: 0.4706"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 19:21:32.931853: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 67s 17s/step - loss: -1443481.2500 - bce: -176006.1094 - dice_coeff: 1355482.6250 - iou: 0.4769 - val_loss: -1108305.0000 - val_bce: -195332.4688 - val_dice_coeff: 1010642.8125 - val_iou: 0.2508 - lr: 9.9100e-05\n",
      "\n",
      "Epoch 3: LearningRateScheduler setting learning rate to 9.819818665965754e-05.\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 19:21:53.380873: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/4 [=====================>........] - ETA: 15s - loss: -1490391.7500 - bce: -180446.7656 - dice_coeff: 1400175.3750 - iou: 0.4910"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 19:22:41.203883: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 67s 17s/step - loss: -1500260.7500 - bce: -184054.2656 - dice_coeff: 1408239.6250 - iou: 0.4981 - val_loss: -1011897.1875 - val_bce: -220610.1250 - val_dice_coeff: 901598.1875 - val_iou: 0.2852 - lr: 9.8198e-05\n",
      "\n",
      "Epoch 4: LearningRateScheduler setting learning rate to 9.729590473501306e-05.\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 19:23:00.632785: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/4 [=====================>........] - ETA: 15s - loss: -1594099.7500 - bce: -175509.0781 - dice_coeff: 1506349.3750 - iou: 0.4792"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 19:23:47.756617: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 66s 17s/step - loss: -1586317.5000 - bce: -174926.0781 - dice_coeff: 1498859.0000 - iou: 0.4788 - val_loss: -1102486.7500 - val_bce: -217660.9844 - val_dice_coeff: 993662.9375 - val_iou: 0.2726 - lr: 9.7296e-05\n",
      "\n",
      "Epoch 5: LearningRateScheduler setting learning rate to 9.63926921258551e-05.\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 19:24:06.817214: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/4 [=====================>........] - ETA: 15s - loss: -1740754.1250 - bce: -182009.4219 - dice_coeff: 1649754.5000 - iou: 0.5016"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 19:24:53.131242: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 65s 17s/step - loss: -1736515.6250 - bce: -182202.6562 - dice_coeff: 1645418.5000 - iou: 0.5014 - val_loss: -1207821.3750 - val_bce: -228556.3906 - val_dice_coeff: 1093550.2500 - val_iou: 0.2776 - lr: 9.6393e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 19:25:12.124293: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for client  2\n",
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 0.0001.\n",
      "Epoch 1/5\n",
      "3/4 [=====================>........] - ETA: 15s - loss: -1782218.8750 - bce: -186712.5156 - dice_coeff: 1688866.3750 - iou: 0.5201"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 19:26:06.555209: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 73s 17s/step - loss: -1788248.3750 - bce: -185219.7188 - dice_coeff: 1695642.2500 - iou: 0.5203 - val_loss: -1160269.8750 - val_bce: -217940.4688 - val_dice_coeff: 1051304.7500 - val_iou: 0.2667 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 2: LearningRateScheduler setting learning rate to 9.909954834128343e-05.\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 19:26:26.707510: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/4 [=====================>........] - ETA: 15s - loss: -1891072.1250 - bce: -180203.4688 - dice_coeff: 1800974.6250 - iou: 0.5130"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 19:27:13.136620: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 66s 17s/step - loss: -1897448.0000 - bce: -182963.3594 - dice_coeff: 1805969.5000 - iou: 0.5193 - val_loss: -1173818.3750 - val_bce: -214695.5469 - val_dice_coeff: 1066476.8750 - val_iou: 0.2569 - lr: 9.9100e-05\n",
      "\n",
      "Epoch 3: LearningRateScheduler setting learning rate to 9.819818665965754e-05.\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 19:27:32.525225: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/4 [=====================>........] - ETA: 15s - loss: -1993872.0000 - bce: -183899.1875 - dice_coeff: 1901925.0000 - iou: 0.5210"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 19:28:19.038642: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 66s 17s/step - loss: -1997316.3750 - bce: -182851.1250 - dice_coeff: 1905895.0000 - iou: 0.5177 - val_loss: -1198690.5000 - val_bce: -221915.4844 - val_dice_coeff: 1087737.0000 - val_iou: 0.2638 - lr: 9.8198e-05\n",
      "\n",
      "Epoch 4: LearningRateScheduler setting learning rate to 9.729590473501306e-05.\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 19:28:38.196145: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/4 [=====================>........] - ETA: 15s - loss: -2036576.3750 - bce: -179581.0156 - dice_coeff: 1946793.3750 - iou: 0.5224"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 19:29:25.277290: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 66s 17s/step - loss: -2058402.3750 - bce: -179394.8750 - dice_coeff: 1968711.7500 - iou: 0.5231 - val_loss: -1162930.7500 - val_bce: -177758.4844 - val_dice_coeff: 1074056.8750 - val_iou: 0.2096 - lr: 9.7296e-05\n",
      "\n",
      "Epoch 5: LearningRateScheduler setting learning rate to 9.63926921258551e-05.\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 19:29:44.564050: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/4 [=====================>........] - ETA: 15s - loss: -2126678.2500 - bce: -182276.7500 - dice_coeff: 2035543.6250 - iou: 0.5313"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 19:30:30.986620: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 66s 17s/step - loss: -2124012.0000 - bce: -183762.8906 - dice_coeff: 2032134.0000 - iou: 0.5345 - val_loss: -1068240.2500 - val_bce: -220253.0625 - val_dice_coeff: 958116.6875 - val_iou: 0.2614 - lr: 9.6393e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 19:30:50.564674: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for client  3\n",
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 0.0001.\n",
      "Epoch 1/5\n",
      "3/4 [=====================>........] - ETA: 15s - loss: -2348404.7500 - bce: -175277.6719 - dice_coeff: 2260772.7500 - iou: 0.5102"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 19:31:45.066258: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 73s 17s/step - loss: -2330137.0000 - bce: -183378.7188 - dice_coeff: 2238455.0000 - iou: 0.5306 - val_loss: -1069053.3750 - val_bce: -182282.2500 - val_dice_coeff: 977919.1250 - val_iou: 0.2154 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 2: LearningRateScheduler setting learning rate to 9.909954834128343e-05.\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 19:32:05.049183: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/4 [=====================>........] - ETA: 15s - loss: -2441918.5000 - bce: -171726.5156 - dice_coeff: 2356063.7500 - iou: 0.5206"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 19:32:51.541539: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 65s 17s/step - loss: -2428304.0000 - bce: -174573.0000 - dice_coeff: 2341025.7500 - iou: 0.5263 - val_loss: -1067168.3750 - val_bce: -198139.2031 - val_dice_coeff: 968103.2500 - val_iou: 0.2335 - lr: 9.9100e-05\n",
      "\n",
      "Epoch 3: LearningRateScheduler setting learning rate to 9.819818665965754e-05.\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 19:33:10.636988: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/4 [=====================>........] - ETA: 16s - loss: -2586096.2500 - bce: -171301.1875 - dice_coeff: 2500451.2500 - iou: 0.5245"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 19:33:58.606171: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 67s 17s/step - loss: -2569818.5000 - bce: -176671.6562 - dice_coeff: 2481488.0000 - iou: 0.5423 - val_loss: -1028629.6875 - val_bce: -180829.1562 - val_dice_coeff: 938221.1875 - val_iou: 0.2191 - lr: 9.8198e-05\n",
      "\n",
      "Epoch 4: LearningRateScheduler setting learning rate to 9.729590473501306e-05.\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 19:34:17.562176: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/4 [=====================>........] - ETA: 15s - loss: -2774084.7500 - bce: -166594.0469 - dice_coeff: 2690791.7500 - iou: 0.5141"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 19:35:03.545376: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 65s 17s/step - loss: -2728603.5000 - bce: -168697.4844 - dice_coeff: 2644259.0000 - iou: 0.5219 - val_loss: -1159664.8750 - val_bce: -187386.9062 - val_dice_coeff: 1065976.2500 - val_iou: 0.2240 - lr: 9.7296e-05\n",
      "\n",
      "Epoch 5: LearningRateScheduler setting learning rate to 9.63926921258551e-05.\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 19:35:22.749321: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/4 [=====================>........] - ETA: 15s - loss: -2843305.0000 - bce: -174448.3750 - dice_coeff: 2756087.7500 - iou: 0.5495"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 19:36:09.399794: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 66s 17s/step - loss: -2906013.5000 - bce: -175360.1094 - dice_coeff: 2818339.0000 - iou: 0.5532 - val_loss: -963848.1250 - val_bce: -176146.1250 - val_dice_coeff: 875778.5000 - val_iou: 0.2153 - lr: 9.6393e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 19:36:28.360807: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for client  4\n",
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 0.0001.\n",
      "Epoch 1/5\n",
      "3/4 [=====================>........] - ETA: 15s - loss: -2796871.5000 - bce: -178522.0469 - dice_coeff: 2707614.7500 - iou: 0.5714"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 19:37:23.220420: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 73s 17s/step - loss: -2823573.5000 - bce: -178854.5156 - dice_coeff: 2734149.5000 - iou: 0.5718 - val_loss: -995383.3750 - val_bce: -196862.8594 - val_dice_coeff: 896957.7500 - val_iou: 0.2471 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 2: LearningRateScheduler setting learning rate to 9.909954834128343e-05.\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 19:37:43.423175: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/4 [=====================>........] - ETA: 15s - loss: -2838374.7500 - bce: -188699.4531 - dice_coeff: 2744029.2500 - iou: 0.6005"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 19:38:29.923547: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 66s 17s/step - loss: -2847732.7500 - bce: -186343.5938 - dice_coeff: 2754564.5000 - iou: 0.5941 - val_loss: -998630.8750 - val_bce: -221693.1875 - val_dice_coeff: 887791.5625 - val_iou: 0.2795 - lr: 9.9100e-05\n",
      "\n",
      "Epoch 3: LearningRateScheduler setting learning rate to 9.819818665965754e-05.\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 19:38:49.128089: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/4 [=====================>........] - ETA: 15s - loss: -3076517.5000 - bce: -196666.6250 - dice_coeff: 2978189.7500 - iou: 0.6222"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 19:39:35.561328: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 65s 17s/step - loss: -3080996.7500 - bce: -188448.6094 - dice_coeff: 2986776.5000 - iou: 0.5978 - val_loss: -1102881.8750 - val_bce: -180323.3594 - val_dice_coeff: 1012723.7500 - val_iou: 0.2305 - lr: 9.8198e-05\n",
      "\n",
      "Epoch 4: LearningRateScheduler setting learning rate to 9.729590473501306e-05.\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 19:39:54.642365: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/4 [=====================>........] - ETA: 15s - loss: -3226331.5000 - bce: -185629.7812 - dice_coeff: 3133521.2500 - iou: 0.5986"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 19:40:40.752118: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 65s 17s/step - loss: -3276249.0000 - bce: -183340.9688 - dice_coeff: 3184581.5000 - iou: 0.5954 - val_loss: -1138814.5000 - val_bce: -212858.3281 - val_dice_coeff: 1032388.9375 - val_iou: 0.2785 - lr: 9.7296e-05\n",
      "\n",
      "Epoch 5: LearningRateScheduler setting learning rate to 9.63926921258551e-05.\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 19:41:00.137249: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/4 [=====================>........] - ETA: 15s - loss: -3240546.7500 - bce: -184586.2812 - dice_coeff: 3148258.0000 - iou: 0.6118"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 19:41:46.386036: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 65s 17s/step - loss: -3297502.5000 - bce: -180650.1562 - dice_coeff: 3207182.5000 - iou: 0.6028 - val_loss: -1310845.2500 - val_bce: -217439.9688 - val_dice_coeff: 1202128.6250 - val_iou: 0.2915 - lr: 9.6393e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 19:42:05.471537: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for client  0\n",
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 0.0001.\n",
      "Epoch 1/5\n",
      "3/4 [=====================>........] - ETA: 15s - loss: -3388658.2500 - bce: -172554.2031 - dice_coeff: 3302382.0000 - iou: 0.5969"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 19:43:16.417701: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 72s 17s/step - loss: -3400048.7500 - bce: -172453.4531 - dice_coeff: 3313822.5000 - iou: 0.6011 - val_loss: -1252092.6250 - val_bce: -205360.1406 - val_dice_coeff: 1149418.1250 - val_iou: 0.2846 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 2: LearningRateScheduler setting learning rate to 9.909954834128343e-05.\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 19:43:36.534269: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/4 [=====================>........] - ETA: 15s - loss: -3647541.7500 - bce: -174988.2656 - dice_coeff: 3560055.7500 - iou: 0.6105"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 19:44:22.991066: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 65s 17s/step - loss: -3664831.2500 - bce: -174448.2812 - dice_coeff: 3577615.0000 - iou: 0.6106 - val_loss: -1012075.1250 - val_bce: -193528.8125 - val_dice_coeff: 915314.5625 - val_iou: 0.2764 - lr: 9.9100e-05\n",
      "\n",
      "Epoch 3: LearningRateScheduler setting learning rate to 9.819818665965754e-05.\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 19:44:42.164437: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/4 [=====================>........] - ETA: 15s - loss: -3597519.5000 - bce: -180272.7969 - dice_coeff: 3507388.2500 - iou: 0.6276"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 19:45:28.838588: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 66s 17s/step - loss: -3647932.7500 - bce: -173418.9531 - dice_coeff: 3561228.0000 - iou: 0.6072 - val_loss: -1014060.1875 - val_bce: -202023.0781 - val_dice_coeff: 913054.1250 - val_iou: 0.3013 - lr: 9.8198e-05\n",
      "\n",
      "Epoch 4: LearningRateScheduler setting learning rate to 9.729590473501306e-05.\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 19:45:47.647305: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/4 [=====================>........] - ETA: 15s - loss: -3938000.7500 - bce: -183263.8281 - dice_coeff: 3846372.7500 - iou: 0.6240"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 19:46:34.045004: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 65s 17s/step - loss: -3770990.5000 - bce: -177906.5156 - dice_coeff: 3682040.0000 - iou: 0.6128 - val_loss: -1004673.1250 - val_bce: -187286.5312 - val_dice_coeff: 911035.3750 - val_iou: 0.2829 - lr: 9.7296e-05\n",
      "\n",
      "Epoch 5: LearningRateScheduler setting learning rate to 9.63926921258551e-05.\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 19:46:53.037139: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/4 [=====================>........] - ETA: 15s - loss: -3954663.5000 - bce: -181510.0312 - dice_coeff: 3863911.2500 - iou: 0.6223"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 19:47:39.269482: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 65s 17s/step - loss: -3847476.7500 - bce: -185214.0625 - dice_coeff: 3754872.0000 - iou: 0.6320 - val_loss: -1337221.7500 - val_bce: -194190.1406 - val_dice_coeff: 1240129.3750 - val_iou: 0.2856 - lr: 9.6393e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 19:47:58.210218: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for client  1\n",
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 0.0001.\n",
      "Epoch 1/5\n",
      "3/4 [=====================>........] - ETA: 15s - loss: -3871155.5000 - bce: -173497.1562 - dice_coeff: 3784413.2500 - iou: 0.5940"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 19:48:52.379749: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 72s 17s/step - loss: -3928842.0000 - bce: -176879.7969 - dice_coeff: 3840409.2500 - iou: 0.6014 - val_loss: -1455311.1250 - val_bce: -185213.7656 - val_dice_coeff: 1362710.3750 - val_iou: 0.2753 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 2: LearningRateScheduler setting learning rate to 9.909954834128343e-05.\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 19:49:12.460557: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/4 [=====================>........] - ETA: 15s - loss: -4057588.7500 - bce: -183650.3906 - dice_coeff: 3965771.2500 - iou: 0.6218"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 19:49:58.882380: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 65s 17s/step - loss: -4076772.0000 - bce: -181577.1562 - dice_coeff: 3985988.7500 - iou: 0.6155 - val_loss: -1758951.2500 - val_bce: -219353.9844 - val_dice_coeff: 1649275.7500 - val_iou: 0.3127 - lr: 9.9100e-05\n",
      "\n",
      "Epoch 3: LearningRateScheduler setting learning rate to 9.819818665965754e-05.\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 19:50:17.994254: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/4 [=====================>........] - ETA: 16s - loss: -4055489.0000 - bce: -185485.3438 - dice_coeff: 3962750.0000 - iou: 0.6352"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 19:51:06.566790: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 67s 17s/step - loss: -3987591.5000 - bce: -180124.4531 - dice_coeff: 3897532.5000 - iou: 0.6233 - val_loss: -2107149.7500 - val_bce: -164948.7188 - val_dice_coeff: 2024683.2500 - val_iou: 0.2479 - lr: 9.8198e-05\n",
      "\n",
      "Epoch 4: LearningRateScheduler setting learning rate to 9.729590473501306e-05.\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 19:51:25.471492: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/4 [=====================>........] - ETA: 15s - loss: -4187260.2500 - bce: -178725.0781 - dice_coeff: 4097902.7500 - iou: 0.6258"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 19:52:11.840756: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 66s 17s/step - loss: -4227305.0000 - bce: -180557.3594 - dice_coeff: 4137032.5000 - iou: 0.6275 - val_loss: -2552282.5000 - val_bce: -198956.2812 - val_dice_coeff: 2452812.0000 - val_iou: 0.3027 - lr: 9.7296e-05\n",
      "\n",
      "Epoch 5: LearningRateScheduler setting learning rate to 9.63926921258551e-05.\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 19:52:31.115730: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/4 [=====================>........] - ETA: 15s - loss: -4177789.2500 - bce: -175232.4531 - dice_coeff: 4090178.0000 - iou: 0.6049"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 19:53:17.302552: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 66s 17s/step - loss: -4308351.0000 - bce: -181838.8906 - dice_coeff: 4217435.0000 - iou: 0.6184 - val_loss: -2213644.0000 - val_bce: -215715.2812 - val_dice_coeff: 2105787.0000 - val_iou: 0.3319 - lr: 9.6393e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 19:53:36.759769: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for client  2\n",
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 0.0001.\n",
      "Epoch 1/5\n",
      "3/4 [=====================>........] - ETA: 15s - loss: -4249387.5000 - bce: -185143.8125 - dice_coeff: 4156820.7500 - iou: 0.6213"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 19:54:31.781402: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 72s 17s/step - loss: -4300097.0000 - bce: -183600.5781 - dice_coeff: 4208300.0000 - iou: 0.6142 - val_loss: -3038003.0000 - val_bce: -217831.8281 - val_dice_coeff: 2929095.5000 - val_iou: 0.3075 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 2: LearningRateScheduler setting learning rate to 9.909954834128343e-05.\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 19:54:51.756742: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/4 [=====================>........] - ETA: 15s - loss: -4512889.5000 - bce: -183851.8906 - dice_coeff: 4420968.5000 - iou: 0.6175"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 19:55:38.041836: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 65s 17s/step - loss: -4408469.0000 - bce: -181430.1719 - dice_coeff: 4317759.5000 - iou: 0.6128 - val_loss: -3003364.7500 - val_bce: -199707.9062 - val_dice_coeff: 2903518.2500 - val_iou: 0.2924 - lr: 9.9100e-05\n",
      "\n",
      "Epoch 3: LearningRateScheduler setting learning rate to 9.819818665965754e-05.\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 19:55:57.330617: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/4 [=====================>........] - ETA: 15s - loss: -4633290.5000 - bce: -177755.1094 - dice_coeff: 4544415.5000 - iou: 0.6098"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 19:56:43.804092: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 66s 17s/step - loss: -4530413.0000 - bce: -176729.9219 - dice_coeff: 4442049.0000 - iou: 0.6109 - val_loss: -2939888.7500 - val_bce: -202293.7188 - val_dice_coeff: 2838745.0000 - val_iou: 0.3118 - lr: 9.8198e-05\n",
      "\n",
      "Epoch 4: LearningRateScheduler setting learning rate to 9.729590473501306e-05.\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 19:57:02.807085: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/4 [=====================>........] - ETA: 15s - loss: -4536095.0000 - bce: -178346.0625 - dice_coeff: 4446924.0000 - iou: 0.6098"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 19:57:50.149657: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 66s 17s/step - loss: -4582050.5000 - bce: -183519.3125 - dice_coeff: 4490291.5000 - iou: 0.6196 - val_loss: -3194113.7500 - val_bce: -222314.0625 - val_dice_coeff: 3082962.7500 - val_iou: 0.3532 - lr: 9.7296e-05\n",
      "\n",
      "Epoch 5: LearningRateScheduler setting learning rate to 9.63926921258551e-05.\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 19:58:09.224318: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/4 [=====================>........] - ETA: 15s - loss: -4706018.0000 - bce: -185669.7031 - dice_coeff: 4613188.0000 - iou: 0.6176"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 19:58:55.214449: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 65s 17s/step - loss: -4775571.0000 - bce: -187677.4219 - dice_coeff: 4681738.5000 - iou: 0.6223 - val_loss: -3747565.2500 - val_bce: -171722.3281 - val_dice_coeff: 3661712.2500 - val_iou: 0.2781 - lr: 9.6393e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 19:59:14.491272: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for client  3\n",
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 0.0001.\n",
      "Epoch 1/5\n",
      "3/4 [=====================>........] - ETA: 15s - loss: -5014532.5000 - bce: -170778.0156 - dice_coeff: 4929144.5000 - iou: 0.5792"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 20:00:08.670868: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 72s 17s/step - loss: -4933017.0000 - bce: -178265.4688 - dice_coeff: 4843887.0000 - iou: 0.6001 - val_loss: -3731540.5000 - val_bce: -177197.0312 - val_dice_coeff: 3642950.5000 - val_iou: 0.2585 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 2: LearningRateScheduler setting learning rate to 9.909954834128343e-05.\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 20:00:28.727264: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/4 [=====================>........] - ETA: 15s - loss: -4902764.5000 - bce: -181326.1562 - dice_coeff: 4812098.0000 - iou: 0.6182"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 20:01:15.188387: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 66s 17s/step - loss: -5009456.5000 - bce: -172658.1562 - dice_coeff: 4923129.0000 - iou: 0.5882 - val_loss: -3445964.7500 - val_bce: -207006.3750 - val_dice_coeff: 3342470.7500 - val_iou: 0.3497 - lr: 9.9100e-05\n",
      "\n",
      "Epoch 3: LearningRateScheduler setting learning rate to 9.819818665965754e-05.\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 20:01:34.482851: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/4 [=====================>........] - ETA: 15s - loss: -5227998.0000 - bce: -172595.6562 - dice_coeff: 5141706.5000 - iou: 0.5934"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 20:02:21.718716: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 66s 17s/step - loss: -5194613.5000 - bce: -164940.1719 - dice_coeff: 5112148.0000 - iou: 0.5711 - val_loss: -4075548.0000 - val_bce: -186205.2500 - val_dice_coeff: 3982443.2500 - val_iou: 0.2920 - lr: 9.8198e-05\n",
      "\n",
      "Epoch 4: LearningRateScheduler setting learning rate to 9.729590473501306e-05.\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 20:02:40.889017: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/4 [=====================>........] - ETA: 15s - loss: -5154597.0000 - bce: -170955.1250 - dice_coeff: 5069128.0000 - iou: 0.5896"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 20:03:27.508457: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 66s 17s/step - loss: -5169965.5000 - bce: -177299.0625 - dice_coeff: 5081324.0000 - iou: 0.6094 - val_loss: -4983972.5000 - val_bce: -182164.6719 - val_dice_coeff: 4892896.0000 - val_iou: 0.2548 - lr: 9.7296e-05\n",
      "\n",
      "Epoch 5: LearningRateScheduler setting learning rate to 9.63926921258551e-05.\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 20:03:46.577777: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/4 [=====================>........] - ETA: 15s - loss: -5553381.5000 - bce: -168785.3438 - dice_coeff: 5468986.5000 - iou: 0.5731"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 20:04:32.562475: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 66s 17s/step - loss: -5428309.0000 - bce: -175431.5469 - dice_coeff: 5340595.0000 - iou: 0.5925 - val_loss: -5167464.5000 - val_bce: -181975.3594 - val_dice_coeff: 5076474.5000 - val_iou: 0.2560 - lr: 9.6393e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 20:04:52.104558: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for client  4\n",
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 0.0001.\n",
      "Epoch 1/5\n",
      "3/4 [=====================>........] - ETA: 15s - loss: -5258277.5000 - bce: -179483.7188 - dice_coeff: 5168537.5000 - iou: 0.5927"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 20:05:46.382328: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 73s 17s/step - loss: -5337479.5000 - bce: -186115.5156 - dice_coeff: 5244423.5000 - iou: 0.6091 - val_loss: -4694946.5000 - val_bce: -204002.9219 - val_dice_coeff: 4592949.0000 - val_iou: 0.2950 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 2: LearningRateScheduler setting learning rate to 9.909954834128343e-05.\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 20:06:06.862937: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/4 [=====================>........] - ETA: 15s - loss: -5432390.0000 - bce: -181921.8594 - dice_coeff: 5341428.5000 - iou: 0.5870"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 20:06:53.182267: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 65s 17s/step - loss: -5377003.5000 - bce: -188694.2969 - dice_coeff: 5282656.5000 - iou: 0.6073 - val_loss: -4198171.0000 - val_bce: -192013.8906 - val_dice_coeff: 4102175.5000 - val_iou: 0.2804 - lr: 9.9100e-05\n",
      "\n",
      "Epoch 3: LearningRateScheduler setting learning rate to 9.819818665965754e-05.\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 20:07:12.345975: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/4 [=====================>........] - ETA: 15s - loss: -5309286.5000 - bce: -198489.3750 - dice_coeff: 5210046.5000 - iou: 0.6376"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 20:07:58.759347: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 66s 17s/step - loss: -5310438.0000 - bce: -195940.3906 - dice_coeff: 5212472.0000 - iou: 0.6298 - val_loss: -3903125.5000 - val_bce: -184881.0312 - val_dice_coeff: 3810692.0000 - val_iou: 0.2961 - lr: 9.8198e-05\n",
      "\n",
      "Epoch 4: LearningRateScheduler setting learning rate to 9.729590473501306e-05.\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 20:08:17.844617: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/4 [=====================>........] - ETA: 15s - loss: -5291636.5000 - bce: -195952.1094 - dice_coeff: 5193665.5000 - iou: 0.6232"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 20:09:04.230643: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 65s 17s/step - loss: -5412941.0000 - bce: -191530.6719 - dice_coeff: 5317182.5000 - iou: 0.6093 - val_loss: -4284143.0000 - val_bce: -192026.5156 - val_dice_coeff: 4188133.0000 - val_iou: 0.3514 - lr: 9.7296e-05\n",
      "\n",
      "Epoch 5: LearningRateScheduler setting learning rate to 9.63926921258551e-05.\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 20:09:23.248498: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/4 [=====================>........] - ETA: 15s - loss: -5467833.5000 - bce: -197951.8438 - dice_coeff: 5368864.0000 - iou: 0.6249"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 20:10:09.575199: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 65s 17s/step - loss: -5481770.0000 - bce: -193742.2344 - dice_coeff: 5384904.0000 - iou: 0.6132 - val_loss: -5233022.0000 - val_bce: -174567.0781 - val_dice_coeff: 5145749.0000 - val_iou: 0.3645 - lr: 9.6393e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 20:10:28.503663: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for client  0\n",
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 0.0001.\n",
      "Epoch 1/5\n",
      "3/4 [=====================>........] - ETA: 16s - loss: -5604712.0000 - bce: -189455.1562 - dice_coeff: 5509988.0000 - iou: 0.6041"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 20:11:41.995393: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 77s 18s/step - loss: -5554923.0000 - bce: -184497.5156 - dice_coeff: 5462679.0000 - iou: 0.5913 - val_loss: -5987577.5000 - val_bce: -192472.6562 - val_dice_coeff: 5891355.0000 - val_iou: 0.5418 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 2: LearningRateScheduler setting learning rate to 9.909954834128343e-05.\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 20:12:04.855909: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/4 [=====================>........] - ETA: 15s - loss: -5591981.5000 - bce: -182676.5469 - dice_coeff: 5500649.5000 - iou: 0.5909"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 20:12:52.578968: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 67s 17s/step - loss: -5613008.0000 - bce: -178564.9688 - dice_coeff: 5523733.0000 - iou: 0.5785 - val_loss: -6066296.5000 - val_bce: -189440.4688 - val_dice_coeff: 5971572.0000 - val_iou: 0.5529 - lr: 9.9100e-05\n",
      "\n",
      "Epoch 3: LearningRateScheduler setting learning rate to 9.819818665965754e-05.\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 20:13:11.645134: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/4 [=====================>........] - ETA: 15s - loss: -5690765.5000 - bce: -186992.3750 - dice_coeff: 5597274.0000 - iou: 0.6059"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 20:13:57.944924: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 65s 17s/step - loss: -5755853.5000 - bce: -178776.6719 - dice_coeff: 5666471.5000 - iou: 0.5833 - val_loss: -6613020.0000 - val_bce: -176461.5312 - val_dice_coeff: 6524782.0000 - val_iou: 0.5305 - lr: 9.8198e-05\n",
      "\n",
      "Epoch 4: LearningRateScheduler setting learning rate to 9.729590473501306e-05.\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 20:14:17.119984: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/4 [=====================>........] - ETA: 15s - loss: -5824069.0000 - bce: -185650.2344 - dice_coeff: 5731249.5000 - iou: 0.6046"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 20:15:03.145366: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 65s 17s/step - loss: -5824138.5000 - bce: -185150.5312 - dice_coeff: 5731569.0000 - iou: 0.6052 - val_loss: -7566341.5000 - val_bce: -194651.3594 - val_dice_coeff: 7469014.0000 - val_iou: 0.5739 - lr: 9.7296e-05\n",
      "\n",
      "Epoch 5: LearningRateScheduler setting learning rate to 9.63926921258551e-05.\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 20:15:22.173268: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/4 [=====================>........] - ETA: 15s - loss: -5877395.0000 - bce: -186113.8281 - dice_coeff: 5784342.5000 - iou: 0.6092"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 20:16:09.312553: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 66s 17s/step - loss: -5819819.0000 - bce: -181747.7969 - dice_coeff: 5728953.0000 - iou: 0.5995 - val_loss: -9311601.0000 - val_bce: -212654.6250 - val_dice_coeff: 9205292.0000 - val_iou: 0.5856 - lr: 9.6393e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 20:16:28.388609: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for client  1\n",
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 0.0001.\n",
      "Epoch 1/5\n",
      "3/4 [=====================>........] - ETA: 15s - loss: -5759280.0000 - bce: -182874.1875 - dice_coeff: 5667844.0000 - iou: 0.6012"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 20:17:22.401984: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 72s 17s/step - loss: -5831586.5000 - bce: -183547.8281 - dice_coeff: 5739815.0000 - iou: 0.5992 - val_loss: -7413758.5000 - val_bce: -164864.7500 - val_dice_coeff: 7331328.0000 - val_iou: 0.5064 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 2: LearningRateScheduler setting learning rate to 9.909954834128343e-05.\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 20:17:42.403663: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/4 [=====================>........] - ETA: 15s - loss: -5964339.5000 - bce: -186140.4062 - dice_coeff: 5871268.5000 - iou: 0.5955"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 20:18:29.178968: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 66s 17s/step - loss: -5973434.5000 - bce: -187714.6562 - dice_coeff: 5879575.5000 - iou: 0.5995 - val_loss: -7830683.0000 - val_bce: -192815.3125 - val_dice_coeff: 7734282.5000 - val_iou: 0.5738 - lr: 9.9100e-05\n",
      "\n",
      "Epoch 3: LearningRateScheduler setting learning rate to 9.819818665965754e-05.\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 20:18:48.223882: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/4 [=====================>........] - ETA: 15s - loss: -6113838.5000 - bce: -192096.6719 - dice_coeff: 6017796.0000 - iou: 0.6141"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 20:19:34.546806: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 65s 17s/step - loss: -6182553.5000 - bce: -191868.8906 - dice_coeff: 6086621.5000 - iou: 0.6125 - val_loss: -7377697.0000 - val_bce: -208712.7812 - val_dice_coeff: 7273336.5000 - val_iou: 0.6045 - lr: 9.8198e-05\n",
      "\n",
      "Epoch 4: LearningRateScheduler setting learning rate to 9.729590473501306e-05.\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 20:19:53.710892: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/4 [=====================>........] - ETA: 15s - loss: -6099710.0000 - bce: -183796.1094 - dice_coeff: 6007819.5000 - iou: 0.5984"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 20:20:40.617381: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 66s 17s/step - loss: -6074425.0000 - bce: -187484.4688 - dice_coeff: 5980689.0000 - iou: 0.6085 - val_loss: -7029721.0000 - val_bce: -194053.1406 - val_dice_coeff: 6932691.5000 - val_iou: 0.5881 - lr: 9.7296e-05\n",
      "\n",
      "Epoch 5: LearningRateScheduler setting learning rate to 9.63926921258551e-05.\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 20:21:00.001550: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/4 [=====================>........] - ETA: 15s - loss: -6026634.5000 - bce: -184274.3281 - dice_coeff: 5934501.5000 - iou: 0.6002"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 20:21:46.302293: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 65s 17s/step - loss: -6199112.5000 - bce: -185843.8594 - dice_coeff: 6106192.5000 - iou: 0.6031 - val_loss: -6497738.5000 - val_bce: -194650.8438 - val_dice_coeff: 6400421.0000 - val_iou: 0.5979 - lr: 9.6393e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 20:22:05.246923: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for client  2\n",
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 0.0001.\n",
      "Epoch 1/5\n",
      "3/4 [=====================>........] - ETA: 15s - loss: -6334425.5000 - bce: -186699.6250 - dice_coeff: 6241086.5000 - iou: 0.6072"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 20:22:59.303274: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 72s 17s/step - loss: -6312459.0000 - bce: -184646.6094 - dice_coeff: 6220146.0000 - iou: 0.6027 - val_loss: -7014713.5000 - val_bce: -198197.8125 - val_dice_coeff: 6915625.0000 - val_iou: 0.6236 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 2: LearningRateScheduler setting learning rate to 9.909954834128343e-05.\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 20:23:19.424142: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/4 [=====================>........] - ETA: 15s - loss: -6436846.0000 - bce: -171243.7812 - dice_coeff: 6351225.5000 - iou: 0.5746"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 20:24:06.022630: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 66s 17s/step - loss: -6458004.0000 - bce: -177343.5781 - dice_coeff: 6369332.5000 - iou: 0.5918 - val_loss: -7844857.5000 - val_bce: -184098.8906 - val_dice_coeff: 7752807.0000 - val_iou: 0.5883 - lr: 9.9100e-05\n",
      "\n",
      "Epoch 3: LearningRateScheduler setting learning rate to 9.819818665965754e-05.\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 20:24:25.095794: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/4 [=====================>........] - ETA: 16s - loss: -6433270.5000 - bce: -181308.9375 - dice_coeff: 6342626.5000 - iou: 0.5980"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 20:25:14.945784: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 69s 18s/step - loss: -6426216.0000 - bce: -183433.9219 - dice_coeff: 6334509.0000 - iou: 0.6034 - val_loss: -6613298.0000 - val_bce: -179066.5469 - val_dice_coeff: 6523757.5000 - val_iou: 0.5945 - lr: 9.8198e-05\n",
      "\n",
      "Epoch 4: LearningRateScheduler setting learning rate to 9.729590473501306e-05.\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 20:25:34.306829: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/4 [=====================>........] - ETA: 15s - loss: -6533866.0000 - bce: -184656.0625 - dice_coeff: 6441541.5000 - iou: 0.5979"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 20:26:20.629838: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 65s 17s/step - loss: -6397645.5000 - bce: -179519.6719 - dice_coeff: 6307886.5000 - iou: 0.5862 - val_loss: -6992641.5000 - val_bce: -182216.2188 - val_dice_coeff: 6901543.5000 - val_iou: 0.5926 - lr: 9.7296e-05\n",
      "\n",
      "Epoch 5: LearningRateScheduler setting learning rate to 9.63926921258551e-05.\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 20:26:39.648041: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/4 [=====================>........] - ETA: 15s - loss: -6666555.0000 - bce: -183548.4531 - dice_coeff: 6574783.5000 - iou: 0.5882"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 20:27:25.802201: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 66s 17s/step - loss: -6698248.0000 - bce: -188673.2344 - dice_coeff: 6603912.0000 - iou: 0.6008 - val_loss: -6792078.5000 - val_bce: -174371.8750 - val_dice_coeff: 6704890.5000 - val_iou: 0.5761 - lr: 9.6393e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 20:27:45.168264: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for client  3\n",
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 0.0001.\n",
      "Epoch 1/5\n",
      "3/4 [=====================>........] - ETA: 15s - loss: -6938109.0000 - bce: -180809.4219 - dice_coeff: 6847710.5000 - iou: 0.5714"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 20:28:40.051020: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 73s 17s/step - loss: -7038914.5000 - bce: -180402.3125 - dice_coeff: 6948722.0000 - iou: 0.5717 - val_loss: -10062336.0000 - val_bce: -169930.2812 - val_dice_coeff: 9977396.0000 - val_iou: 0.5522 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 20:28:59.916788: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2: LearningRateScheduler setting learning rate to 9.909954834128343e-05.\n",
      "Epoch 2/5\n",
      "3/4 [=====================>........] - ETA: 15s - loss: -6932471.5000 - bce: -178136.0156 - dice_coeff: 6843411.5000 - iou: 0.5807"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 20:30:01.444486: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 65s 17s/step - loss: -6917120.0000 - bce: -175174.4375 - dice_coeff: 6829537.5000 - iou: 0.5720 - val_loss: -12581565.0000 - val_bce: -178006.2656 - val_dice_coeff: 12492581.0000 - val_iou: 0.5635 - lr: 9.9100e-05\n",
      "\n",
      "Epoch 3: LearningRateScheduler setting learning rate to 9.819818665965754e-05.\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 20:30:20.309242: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/4 [=====================>........] - ETA: 15s - loss: -7048528.5000 - bce: -170872.4531 - dice_coeff: 6963102.0000 - iou: 0.5705"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 20:31:07.062217: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 66s 17s/step - loss: -6978843.0000 - bce: -171140.6562 - dice_coeff: 6893285.0000 - iou: 0.5698 - val_loss: -13001089.0000 - val_bce: -159109.5781 - val_dice_coeff: 12921525.0000 - val_iou: 0.5014 - lr: 9.8198e-05\n",
      "\n",
      "Epoch 4: LearningRateScheduler setting learning rate to 9.729590473501306e-05.\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 20:31:26.132759: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/4 [=====================>........] - ETA: 15s - loss: -7449878.5000 - bce: -179852.0938 - dice_coeff: 7359961.5000 - iou: 0.5839"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 20:32:12.399615: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 66s 17s/step - loss: -7173878.5000 - bce: -179961.0781 - dice_coeff: 7083903.5000 - iou: 0.5834 - val_loss: -9893978.0000 - val_bce: -187133.9844 - val_dice_coeff: 9800396.0000 - val_iou: 0.5798 - lr: 9.7296e-05\n",
      "\n",
      "Epoch 5: LearningRateScheduler setting learning rate to 9.63926921258551e-05.\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 20:32:31.575646: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/4 [=====================>........] - ETA: 15s - loss: -7157651.0000 - bce: -172468.1094 - dice_coeff: 7071430.0000 - iou: 0.5435"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 20:33:17.841304: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 66s 17s/step - loss: -7135749.5000 - bce: -177585.7656 - dice_coeff: 7046969.0000 - iou: 0.5575 - val_loss: -9203256.0000 - val_bce: -159708.7188 - val_dice_coeff: 9123399.0000 - val_iou: 0.5025 - lr: 9.6393e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 20:33:37.267851: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for client  4\n",
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 0.0001.\n",
      "Epoch 1/5\n",
      "3/4 [=====================>........] - ETA: 15s - loss: -7128329.5000 - bce: -180441.5625 - dice_coeff: 7038112.0000 - iou: 0.5577"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 20:34:31.302913: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 72s 17s/step - loss: -7140472.0000 - bce: -182984.8594 - dice_coeff: 7048979.5000 - iou: 0.5641 - val_loss: -7895403.0000 - val_bce: -172709.1875 - val_dice_coeff: 7809062.5000 - val_iou: 0.5344 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 2: LearningRateScheduler setting learning rate to 9.909954834128343e-05.\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 20:34:51.311147: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/4 [=====================>........] - ETA: 15s - loss: -7363245.5000 - bce: -189227.3594 - dice_coeff: 7268626.5000 - iou: 0.5747"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 20:35:37.795183: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 66s 17s/step - loss: -7294417.5000 - bce: -189378.4219 - dice_coeff: 7199723.5000 - iou: 0.5756 - val_loss: -7163649.5000 - val_bce: -189045.2188 - val_dice_coeff: 7069121.5000 - val_iou: 0.6040 - lr: 9.9100e-05\n",
      "\n",
      "Epoch 3: LearningRateScheduler setting learning rate to 9.819818665965754e-05.\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 20:35:57.108637: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/4 [=====================>........] - ETA: 15s - loss: -7082900.5000 - bce: -189379.8750 - dice_coeff: 6988208.5000 - iou: 0.5763"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 20:36:43.569014: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 65s 17s/step - loss: -7387045.0000 - bce: -187402.2812 - dice_coeff: 7293342.0000 - iou: 0.5691 - val_loss: -7575091.0000 - val_bce: -169881.8750 - val_dice_coeff: 7490144.0000 - val_iou: 0.5344 - lr: 9.8198e-05\n",
      "\n",
      "Epoch 4: LearningRateScheduler setting learning rate to 9.729590473501306e-05.\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 20:37:02.589145: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/4 [=====================>........] - ETA: 15s - loss: -7223343.0000 - bce: -177421.9688 - dice_coeff: 7134637.5000 - iou: 0.5455"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 20:37:48.828597: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 65s 17s/step - loss: -7322493.5000 - bce: -180865.5156 - dice_coeff: 7232063.5000 - iou: 0.5549 - val_loss: -7281849.5000 - val_bce: -151097.6719 - val_dice_coeff: 7206313.5000 - val_iou: 0.5106 - lr: 9.7296e-05\n",
      "\n",
      "Epoch 5: LearningRateScheduler setting learning rate to 9.63926921258551e-05.\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 20:38:08.075118: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/4 [=====================>........] - ETA: 15s - loss: -7446773.5000 - bce: -191985.4375 - dice_coeff: 7350792.5000 - iou: 0.5780"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 20:38:54.155487: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 65s 17s/step - loss: -7401078.5000 - bce: -188722.5625 - dice_coeff: 7306728.0000 - iou: 0.5740 - val_loss: -7071505.5000 - val_bce: -149875.6250 - val_dice_coeff: 6996566.0000 - val_iou: 0.5006 - lr: 9.6393e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 20:39:13.497812: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for client  0\n",
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 0.0001.\n",
      "Epoch 1/5\n",
      "3/4 [=====================>........] - ETA: 16s - loss: -7375079.5000 - bce: -189815.1406 - dice_coeff: 7280178.5000 - iou: 0.5805"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 20:40:26.839320: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 74s 18s/step - loss: -7472270.5000 - bce: -184335.7031 - dice_coeff: 7380110.5000 - iou: 0.5645 - val_loss: -7921789.0000 - val_bce: -173545.8906 - val_dice_coeff: 7835029.0000 - val_iou: 0.5715 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 2: LearningRateScheduler setting learning rate to 9.909954834128343e-05.\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 20:40:46.467505: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/4 [=====================>........] - ETA: 15s - loss: -7838629.5000 - bce: -196874.1875 - dice_coeff: 7740196.0000 - iou: 0.5990"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 20:41:33.309362: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 66s 17s/step - loss: -7861217.0000 - bce: -189228.8125 - dice_coeff: 7766605.0000 - iou: 0.5786 - val_loss: -8862783.0000 - val_bce: -181376.5938 - val_dice_coeff: 8772093.0000 - val_iou: 0.5565 - lr: 9.9100e-05\n",
      "\n",
      "Epoch 3: LearningRateScheduler setting learning rate to 9.819818665965754e-05.\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 20:41:52.669902: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/4 [=====================>........] - ETA: 15s - loss: -7730074.0000 - bce: -197479.4219 - dice_coeff: 7631335.5000 - iou: 0.5889"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 20:42:38.601943: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 65s 17s/step - loss: -7658113.0000 - bce: -189794.5000 - dice_coeff: 7563220.5000 - iou: 0.5699 - val_loss: -10223236.0000 - val_bce: -172846.2500 - val_dice_coeff: 10136800.0000 - val_iou: 0.5305 - lr: 9.8198e-05\n",
      "\n",
      "Epoch 4: LearningRateScheduler setting learning rate to 9.729590473501306e-05.\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 20:42:57.774327: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/4 [=====================>........] - ETA: 15s - loss: -7758485.5000 - bce: -193402.5000 - dice_coeff: 7661790.5000 - iou: 0.5838"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 20:43:44.137861: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 65s 17s/step - loss: -7796321.5000 - bce: -194051.6406 - dice_coeff: 7699298.0000 - iou: 0.5848 - val_loss: -9535014.0000 - val_bce: -187701.7500 - val_dice_coeff: 9441162.0000 - val_iou: 0.5712 - lr: 9.7296e-05\n",
      "\n",
      "Epoch 5: LearningRateScheduler setting learning rate to 9.63926921258551e-05.\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 20:44:03.243249: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/4 [=====================>........] - ETA: 15s - loss: -7982513.0000 - bce: -194898.9531 - dice_coeff: 7885082.5000 - iou: 0.5856"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 20:44:49.660319: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 66s 17s/step - loss: -7843977.5000 - bce: -192309.7812 - dice_coeff: 7747841.5000 - iou: 0.5806 - val_loss: -11236203.0000 - val_bce: -200266.9844 - val_dice_coeff: 11136080.0000 - val_iou: 0.6008 - lr: 9.6393e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 20:45:08.822703: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for client  1\n",
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 0.0001.\n",
      "Epoch 1/5\n",
      "3/4 [=====================>........] - ETA: 15s - loss: -7911122.0000 - bce: -185579.1406 - dice_coeff: 7818346.5000 - iou: 0.5690"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 20:46:03.357610: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 73s 17s/step - loss: -7862673.5000 - bce: -185159.2500 - dice_coeff: 7770110.0000 - iou: 0.5657 - val_loss: -9580719.0000 - val_bce: -187410.5312 - val_dice_coeff: 9487024.0000 - val_iou: 0.5670 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 2: LearningRateScheduler setting learning rate to 9.909954834128343e-05.\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 20:46:23.332479: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/4 [=====================>........] - ETA: 17s - loss: -8035621.5000 - bce: -194252.9375 - dice_coeff: 7938494.5000 - iou: 0.5803"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 20:47:15.281509: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 71s 18s/step - loss: -7956662.5000 - bce: -195153.5625 - dice_coeff: 7859083.0000 - iou: 0.5803 - val_loss: -8409560.0000 - val_bce: -157179.7656 - val_dice_coeff: 8330963.0000 - val_iou: 0.4964 - lr: 9.9100e-05\n",
      "\n",
      "Epoch 3: LearningRateScheduler setting learning rate to 9.819818665965754e-05.\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 20:47:34.493497: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/4 [=====================>........] - ETA: 16s - loss: -8105163.5000 - bce: -197666.8125 - dice_coeff: 8006334.5000 - iou: 0.5728"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 20:48:23.974231: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 69s 17s/step - loss: -7849356.0000 - bce: -188859.8594 - dice_coeff: 7754931.0000 - iou: 0.5548 - val_loss: -9397369.0000 - val_bce: -201505.8281 - val_dice_coeff: 9296608.0000 - val_iou: 0.5874 - lr: 9.8198e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 20:48:43.152533: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4: LearningRateScheduler setting learning rate to 9.729590473501306e-05.\n",
      "Epoch 4/5\n",
      "3/4 [=====================>........] - ETA: 15s - loss: -7918853.0000 - bce: -189095.4844 - dice_coeff: 7824316.0000 - iou: 0.5549"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 20:49:42.812093: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 65s 17s/step - loss: -7926782.5000 - bce: -192822.4531 - dice_coeff: 7830377.0000 - iou: 0.5628 - val_loss: -7509777.0000 - val_bce: -148964.7656 - val_dice_coeff: 7435311.5000 - val_iou: 0.4910 - lr: 9.7296e-05\n",
      "\n",
      "Epoch 5: LearningRateScheduler setting learning rate to 9.63926921258551e-05.\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 20:50:01.813769: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/4 [=====================>........] - ETA: 15s - loss: -8007220.5000 - bce: -184516.2656 - dice_coeff: 7914968.0000 - iou: 0.5426"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 20:50:47.923659: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 67s 17s/step - loss: -7989976.0000 - bce: -186537.0312 - dice_coeff: 7896715.0000 - iou: 0.5473 - val_loss: -8240595.0000 - val_bce: -178784.1250 - val_dice_coeff: 8151229.0000 - val_iou: 0.5641 - lr: 9.6393e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 20:51:08.979813: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for client  2\n",
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 0.0001.\n",
      "Epoch 1/5\n",
      "3/4 [=====================>........] - ETA: 15s - loss: -8309267.0000 - bce: -189127.5312 - dice_coeff: 8214710.5000 - iou: 0.5644"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 20:52:03.189795: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 72s 17s/step - loss: -8300947.0000 - bce: -188114.7812 - dice_coeff: 8206898.0000 - iou: 0.5606 - val_loss: -10415784.0000 - val_bce: -189059.7188 - val_dice_coeff: 10321258.0000 - val_iou: 0.5807 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 2: LearningRateScheduler setting learning rate to 9.909954834128343e-05.\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 20:52:23.381994: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/4 [=====================>........] - ETA: 15s - loss: -8105419.0000 - bce: -176489.0625 - dice_coeff: 8017190.5000 - iou: 0.5502"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 20:53:09.924141: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 66s 17s/step - loss: -8112403.0000 - bce: -178308.7031 - dice_coeff: 8023262.5000 - iou: 0.5541 - val_loss: -9567194.0000 - val_bce: -167946.2656 - val_dice_coeff: 9483208.0000 - val_iou: 0.5471 - lr: 9.9100e-05\n",
      "\n",
      "Epoch 3: LearningRateScheduler setting learning rate to 9.819818665965754e-05.\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 20:53:29.069979: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/4 [=====================>........] - ETA: 15s - loss: -8212229.5000 - bce: -182779.4688 - dice_coeff: 8120845.5000 - iou: 0.5597"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 20:54:15.374881: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 65s 17s/step - loss: -8209364.0000 - bce: -185233.1406 - dice_coeff: 8116755.5000 - iou: 0.5597 - val_loss: -10169084.0000 - val_bce: -198350.1406 - val_dice_coeff: 10069919.0000 - val_iou: 0.5919 - lr: 9.8198e-05\n",
      "\n",
      "Epoch 4: LearningRateScheduler setting learning rate to 9.729590473501306e-05.\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 20:54:34.346394: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/4 [=====================>........] - ETA: 15s - loss: -8156291.0000 - bce: -193602.8750 - dice_coeff: 8059493.5000 - iou: 0.5583"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 20:55:20.445179: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 65s 17s/step - loss: -8251712.0000 - bce: -195374.4375 - dice_coeff: 8154025.0000 - iou: 0.5635 - val_loss: -10200555.0000 - val_bce: -185165.7812 - val_dice_coeff: 10107976.0000 - val_iou: 0.5431 - lr: 9.7296e-05\n",
      "\n",
      "Epoch 5: LearningRateScheduler setting learning rate to 9.63926921258551e-05.\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 20:55:39.693543: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/4 [=====================>........] - ETA: 15s - loss: -8226994.0000 - bce: -188827.4062 - dice_coeff: 8132592.5000 - iou: 0.5453"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 20:56:26.268692: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 66s 17s/step - loss: -8349753.0000 - bce: -189146.8594 - dice_coeff: 8255190.0000 - iou: 0.5436 - val_loss: -10429446.0000 - val_bce: -211443.5156 - val_dice_coeff: 10323743.0000 - val_iou: 0.6049 - lr: 9.6393e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 20:56:45.257304: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for client  3\n",
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 0.0001.\n",
      "Epoch 1/5\n",
      "3/4 [=====================>........] - ETA: 15s - loss: -8517282.0000 - bce: -188993.7188 - dice_coeff: 8422785.0000 - iou: 0.5534"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 20:57:39.488399: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 72s 17s/step - loss: -8633759.0000 - bce: -188792.4688 - dice_coeff: 8539365.0000 - iou: 0.5549 - val_loss: -10587751.0000 - val_bce: -200619.3750 - val_dice_coeff: 10487437.0000 - val_iou: 0.5900 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 2: LearningRateScheduler setting learning rate to 9.909954834128343e-05.\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 20:57:59.470132: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/4 [=====================>........] - ETA: 16s - loss: -8602387.0000 - bce: -188599.4062 - dice_coeff: 8508101.0000 - iou: 0.5594"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 20:58:47.214349: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 69s 18s/step - loss: -8782756.0000 - bce: -178938.0469 - dice_coeff: 8693297.0000 - iou: 0.5317 - val_loss: -11921732.0000 - val_bce: -165524.5156 - val_dice_coeff: 11838959.0000 - val_iou: 0.4975 - lr: 9.9100e-05\n",
      "\n",
      "Epoch 3: LearningRateScheduler setting learning rate to 9.819818665965754e-05.\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 20:59:08.416817: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/4 [=====================>........] - ETA: 15s - loss: -9039266.0000 - bce: -174968.7500 - dice_coeff: 8951797.0000 - iou: 0.5263"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 20:59:56.953523: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 68s 17s/step - loss: -8940936.0000 - bce: -179833.3281 - dice_coeff: 8851028.0000 - iou: 0.5398 - val_loss: -11903861.0000 - val_bce: -163510.0312 - val_dice_coeff: 11822140.0000 - val_iou: 0.5005 - lr: 9.8198e-05\n",
      "\n",
      "Epoch 4: LearningRateScheduler setting learning rate to 9.729590473501306e-05.\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 21:00:16.112143: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/4 [=====================>........] - ETA: 15s - loss: -9123601.0000 - bce: -173600.5156 - dice_coeff: 9036812.0000 - iou: 0.5334"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 21:01:02.409763: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 66s 17s/step - loss: -8988066.0000 - bce: -176014.2188 - dice_coeff: 8900065.0000 - iou: 0.5435 - val_loss: -9592324.0000 - val_bce: -146893.0312 - val_dice_coeff: 9518907.0000 - val_iou: 0.5104 - lr: 9.7296e-05\n",
      "\n",
      "Epoch 5: LearningRateScheduler setting learning rate to 9.63926921258551e-05.\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 21:01:21.667941: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/4 [=====================>........] - ETA: 15s - loss: -8959582.0000 - bce: -176840.6875 - dice_coeff: 8871153.0000 - iou: 0.5357"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 21:02:07.926894: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 65s 17s/step - loss: -8998683.0000 - bce: -179313.6719 - dice_coeff: 8909024.0000 - iou: 0.5411 - val_loss: -9035035.0000 - val_bce: -151709.4844 - val_dice_coeff: 8959175.0000 - val_iou: 0.4952 - lr: 9.6393e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 21:02:26.917887: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for client  4\n",
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 0.0001.\n",
      "Epoch 1/5\n",
      "3/4 [=====================>........] - ETA: 15s - loss: -8878726.0000 - bce: -191603.3750 - dice_coeff: 8782923.0000 - iou: 0.5453"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 21:03:20.882147: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 72s 17s/step - loss: -8803233.0000 - bce: -189955.4062 - dice_coeff: 8708258.0000 - iou: 0.5373 - val_loss: -8855207.0000 - val_bce: -152665.7812 - val_dice_coeff: 8778871.0000 - val_iou: 0.4998 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 2: LearningRateScheduler setting learning rate to 9.909954834128343e-05.\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 21:03:40.874035: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/4 [=====================>........] - ETA: 15s - loss: -8853120.0000 - bce: -189422.5000 - dice_coeff: 8758413.0000 - iou: 0.5238"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 21:04:27.309082: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 65s 17s/step - loss: -8965269.0000 - bce: -193456.4219 - dice_coeff: 8868545.0000 - iou: 0.5344 - val_loss: -7250718.5000 - val_bce: -169772.2656 - val_dice_coeff: 7165843.5000 - val_iou: 0.5535 - lr: 9.9100e-05\n",
      "\n",
      "Epoch 3: LearningRateScheduler setting learning rate to 9.819818665965754e-05.\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 21:04:46.317735: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/4 [=====================>........] - ETA: 15s - loss: -8957311.0000 - bce: -196268.0312 - dice_coeff: 8859190.0000 - iou: 0.5439"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 21:05:32.949546: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 66s 17s/step - loss: -8866991.0000 - bce: -193061.6094 - dice_coeff: 8770466.0000 - iou: 0.5356 - val_loss: -9134554.0000 - val_bce: -164887.5156 - val_dice_coeff: 9052101.0000 - val_iou: 0.5024 - lr: 9.8198e-05\n",
      "\n",
      "Epoch 4: LearningRateScheduler setting learning rate to 9.729590473501306e-05.\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 21:05:52.588340: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/4 [=====================>........] - ETA: 15s - loss: -8861801.0000 - bce: -193975.9844 - dice_coeff: 8764825.0000 - iou: 0.5446"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 21:06:38.547164: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 65s 17s/step - loss: -9030600.0000 - bce: -194678.3906 - dice_coeff: 8933272.0000 - iou: 0.5463 - val_loss: -10342793.0000 - val_bce: -189367.3750 - val_dice_coeff: 10248116.0000 - val_iou: 0.5651 - lr: 9.7296e-05\n",
      "\n",
      "Epoch 5: LearningRateScheduler setting learning rate to 9.63926921258551e-05.\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 21:06:57.719673: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/4 [=====================>........] - ETA: 15s - loss: -8966756.0000 - bce: -196009.5781 - dice_coeff: 8868752.0000 - iou: 0.5539"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 21:07:45.102962: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "4/4 [==============================] - ETA: 0s - loss: -9189695.0000 - bce: -194075.7812 - dice_coeff: 9092663.0000 - iou: 0.5456 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 21:08:05.656614: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "4/4 [==============================] - 69s 18s/step - loss: -9189695.0000 - bce: -194075.7812 - dice_coeff: 9092663.0000 - iou: 0.5456 - val_loss: -10207306.0000 - val_bce: -189936.2500 - val_dice_coeff: 10112350.0000 - val_iou: 0.5480 - lr: 9.6393e-05\n"
     ]
    }
   ],
   "source": [
    "#commence global training loop\n",
    "for comm_round in range(comms_round):\n",
    "            \n",
    "    # get the global model's weights - will serve as the initial weights for all local models\n",
    "    global_weights = global_model.get_weights()\n",
    "    \n",
    "    #initial list to collect local model weights after scalling\n",
    "    scaled_local_weight_list = list()\n",
    "\n",
    "\n",
    "    #loop through each client and create new local model\n",
    "    for i in range(5):\n",
    "        print(\"for client \", i)\n",
    "        smlp_local = SimpleMLP2()\n",
    "        local_model = smlp_local.build()\n",
    "        local_model.compile(loss=loss, \n",
    "                      optimizer=optimizer, \n",
    "                      metrics=metrics)\n",
    "        \n",
    "        #set local model weight to the weight of the global model\n",
    "        local_model.set_weights(global_weights)\n",
    "        \n",
    "        #fit local model with client's data\n",
    "        \n",
    "        SGD_model2.fit(trains[i], batch_size=batch_size, epochs=EPOCH, callbacks=[scheduler,earlystop,checkpoint],\n",
    "            validation_data=tests[i], steps_per_epoch=int(0.8*num_files//batch_size), validation_steps=int(0.2*num_files//batch_size), shuffle=True)\n",
    "        \n",
    "        #scale the model weights and add to list\n",
    "        scaling_factor = SCALINGFACTOR\n",
    "        scaled_weights = scale_model_weights(local_model.get_weights(), scaling_factor)\n",
    "        scaled_local_weight_list.append(scaled_weights)\n",
    "        \n",
    "        #clear session to free memory after each communication round\n",
    "        K.clear_session()\n",
    "        \n",
    "    #to get the average over all the local model, we simply take the sum of the scaled weights\n",
    "    average_weights = sum_scaled_weights(scaled_local_weight_list)\n",
    "    \n",
    "    #update global model \n",
    "    global_model.set_weights(average_weights)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b49f6a7",
   "metadata": {},
   "source": [
    "# Testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "cec26be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_image_only = tf.data.Dataset.list_files('/home/user2/Desktop/rumi/mri segmentation/2d_images/*.tif',shuffle=False).map(lambda x: tf.py_function(read_image,[x],[tf.float32]))\n",
    "dataset_mask_only = tf.data.Dataset.list_files('/home/user2/Desktop/rumi/mri segmentation/2d_masks/*.tif',shuffle=False).map(lambda x: tf.py_function(read_mask,[x],[tf.float32]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "f26a5baa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 12s 12s/step\n"
     ]
    }
   ],
   "source": [
    "take = 32\n",
    "num = np.random.randint(0,num_files-take-1)\n",
    "\n",
    "image = np.array(list(dataset_image_only.skip(num).take(take).as_numpy_iterator()))\n",
    "truth = np.array(list(dataset_mask_only.skip(num).take(take).as_numpy_iterator()))\n",
    "pred = global_model.predict(dataset_image_only.skip(num).batch(take).take(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9075eae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [20,200]\n",
    "index = 1\n",
    "for i in range(take):\n",
    "    plt.subplot(take,3,index)\n",
    "    plt.title('image %s'%i)\n",
    "    plt.imshow(image[i,0,:,:,0], cmap='gray')\n",
    "    index += 1\n",
    "    plt.subplot(take,3,index)\n",
    "    plt.title('truth %s'%i)\n",
    "    plt.imshow(truth[i,0,:,:,0], cmap='gray')\n",
    "    index += 1\n",
    "    plt.subplot(take,3,index)\n",
    "    plt.title('pred %s'%i)\n",
    "    plt.imshow(pred[i,:,:,0], cmap='gray')\n",
    "    index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd1e8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "References\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
